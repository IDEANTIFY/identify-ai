import os
import json
from openai import OpenAI
from typing import List, Dict, Any
from dotenv import load_dotenv
import re

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# --- OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
# .env íŒŒì¼ì— ì €ì¥ëœ OPENAI_API_KEYë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
# client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY_ahyun"))

# --- [ë³´ê³ ì„œ 1] ìš”ì•½ ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜ ---
def generate_summary_report(
    query: Dict[str, Any], 
    web_docs: List[Dict], 
    crawling_docs: List[Dict],   # ğŸ‘ˆ ì™¸ë¶€ í¬ë¡¤ë§ (ê¸°ì¡´)
    user_docs: List[Dict],   # ğŸ‘ˆ user_docs(ì¶”ê°€)
    approx_similar_count: int
) -> Dict[str, Any]:
    """
    ì•„ì´ë””ì–´ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìš”ì•½ ë³´ê³ ì„œ(JSON)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    query_str = json.dumps(query, ensure_ascii=False, indent=2) 

    # 1. ì›¹ ë° DB ë¬¸ì„œ ì„¹ì…˜ êµ¬ì„±
    ### ğŸ’¡ ìˆ˜ì •
    web_section = "\n\n".join([
        f"[WEB{i+1}] Title: {doc.get('title', '')}\n"
        f"Snippet: {doc.get('snippet', '')}\n"
        f"Source: {doc.get('source', 'N/A')} (Link: {doc.get('link', 'N/A')})\n"
        for i, doc in enumerate(web_docs)
    ])
    
    # 2. ì •ì  DB ë¬¸ì„œ ì„¹ì…˜ êµ¬ì„± (í¬ë¡¤ë§ DB)
    crawling_section = "\n\n".join([ # ğŸ’¡ ë³€ìˆ˜ëª… ë³€ê²½
        f"[CRAWLING_DB{i+1}] Title: {doc.get('title', 'N/A')}\n" # ğŸ’¡ ë¼ë²¨ ë³€ê²½
        f"Content: {doc.get('content', '')}\n"
        f"Keyword: {doc.get('keyword', 'N/A')}\n"
        f"Team Members: {', '.join(doc.get('team_members', [])) or 'N/A'}\n"
        f"Source: {doc.get('source', 'N/A')} (Link: {doc.get('link', 'N/A')})\n"
        f"Score: {doc.get('score', '')}"
        for i, doc in enumerate(crawling_docs) 
    ])
    # 3. ë¼ì´ë¸Œ DB (ìœ ì € DB) ë¬¸ì„œ ì„¹ì…˜ êµ¬ì„± (user_sectionìœ¼ë¡œ ë³€ê²½)
    user_section = "\n\n".join([ # ğŸ’¡ ë³€ìˆ˜ëª… ë³€ê²½
        f"[USER_DB{i+1}] Title: {doc.get('title', 'N/A')}\n"
        f"Content: {doc.get('content', '')}\n"
        f"Keyword: {doc.get('keyword', 'N/A')}\n"
        f"Team Members: {', '.join(doc.get('team_members', [])) or 'N/A'}\n"
        f"Source: {doc.get('source', 'N/A')} (Link: {doc.get('link', 'N/A')})\n"
        f"Score: {doc.get('score', '')}"
        for i, doc in enumerate(user_docs) 
    ])


    prompt = f"""
ë‹¹ì‹ ì€ ëŒ€í•™ìƒ ìŠ¤íƒ€íŠ¸ì—… ì•„ì´ë””ì–´ë¥¼ ì •ëŸ‰ì  ê¸°ì¤€ì— ë”°ë¼ í‰ê°€í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì‚¬ìš©ì ì•„ì´ë””ì–´]
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤:
{query_str}

[ìœ ì‚¬ ì‚¬ë¡€ ë°ì´í„°]
1. ì›¹ ê²€ìƒ‰ ìƒìœ„ {len(web_docs)}ê°œ ê²°ê³¼:
{web_section}

2. í¬ë¡¤ë§ DB (ê³µëª¨ì „ ë“± ì •ì  ë°ì´í„°):
{crawling_section}

3. ìœ ì € DB (outer_project ì‹¤ì‹œê°„ ë°ì´í„°):
{user_section}

ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ì•„ì´ë””ì–´ë¥¼ í‰ê°€í•˜ê³ , ê²°ê³¼ë¥¼ ì•„ë˜ì— ëª…ì‹œëœ JSON í˜•ì‹ ì˜ˆì‹œì— ì •í™•íˆ ë§ì¶° ì¡´ëŒ“ë§ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.

[í‰ê°€ ê¸°ì¤€]
1.  total_similar_cases: ì›¹ ê²€ìƒ‰ ê²°ê³¼({approx_similar_count}ê±´)ì™€ í¬ë¡¤ë§ DB, ìœ ì € DB ê²°ê³¼ ë¬¸ì„œ ìˆ˜ë¥¼ ì¢…í•©í•˜ì—¬ ì „ì²´ ìœ ì‚¬ ê±´ìˆ˜ë¥¼ ì •ìˆ˜ë¡œ íŒë‹¨.
2.  similarity: í¬ë¡¤ë§ DB + ìœ ì € DB + ì›¹ ë¬¸ì„œ ê¸°ì¤€, ê¸°ìˆ ì  ì»¨ì…‰ì˜ ì¤‘ë³µì„±ì„ íŒë‹¨í•˜ì—¬ 0~100 ì‚¬ì´ ì •ìˆ˜ ê°’ìœ¼ë¡œ í‰ê°€.
3.  creativity: ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ì¡°í•©, ìƒˆë¡œì›€, ë‹¤ë¥¸ ëª©ì /ëŒ€ìƒ ì„¤ì • ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì—¬ 0~100 ì‚¬ì´ ì •ìˆ˜ ê°’ìœ¼ë¡œ í‰ê°€.
4.  feasibility: ê¸°ìˆ  ì„±ìˆ™ë„, ìƒìš©í™” ì—¬ë¶€, êµ¬í˜„ ë‚œì´ë„ ë“±ì„ ê³ ë ¤í•˜ì—¬ 0~100 ì‚¬ì´ ì •ìˆ˜ ê°’ìœ¼ë¡œ í‰ê°€.
5.  analysis_narrative: ìœ„ í‰ê°€ë¥¼ ì¢…í•©í•˜ì—¬ 600ì ë‚´ì™¸ì˜ ë¶„ì„ ìš”ì•½ì„ ë¬¸ìì—´ë¡œ ì‘ì„±.

[ì¶œë ¥ JSON í˜•ì‹ ì˜ˆì‹œ]
```json
{{
  "report_summary": {{
    "total_similar_cases": 19,
    "evaluation_scores": {{
      "similarity": 80,
      "creativity": 50,
      "feasibility": 50
    }},
    "analysis_narrative": "ê³µì¤‘ ëª¨ë¹Œë¦¬í‹° ë„ë©”ì¸ì˜ ë„ì‹¬ í†µê·¼ìš© ìˆ˜ì§ ì´ì°©ë¥™ ììœ¨ ë¹„í–‰ ì œì–´ í•˜ëŠ˜ ë¹„í–‰ ìë™ì°¨ ì•„ì´ë””ì–´ëŠ”, UAM/eVTOL ê´€ë ¨ ë‹¤ìˆ˜ ë¬¸í—Œê³¼ ìƒìš©í™” íë¦„ê³¼ ë†’ì€ ì¤‘ë³µì„±ì„ ë³´ì…ë‹ˆë‹¤. í•µì‹¬ ì»¨ì…‰(ë„ì‹¬ í†µê·¼, ìˆ˜ì§ ì´ì°©ë¥™, ììœ¨ ì œì–´)ì€ ê¸°ì¡´ ì—°êµ¬ì™€ ìƒë‹¹íˆ ê²¹ì¹˜ë‚˜, ëŒ€í•™ìƒ ìŠ¤íƒ€íŠ¸ì—…ì˜ ì°¨ë³„í™” í¬ì¸íŠ¸ê°€ ëª¨í˜¸í•©ë‹ˆë‹¤. ì‹¤í˜„ ê°€ëŠ¥ì„±ì€ ê¸°ìˆ  ì„±ìˆ™ë„ì™€ ê·œì œÂ·ì¸í”„ë¼ ì´ìŠˆë¡œ ì¸í•´ ë³´ìˆ˜ì ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤."
  }}
}}
```
ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ê°ì²´ë§Œ ìƒì„±í•˜ì„¸ìš”. ì´ì œ í‰ê°€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.
"""
    try:
        response = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[{"role": "user", "content": prompt.strip()}]
        )
        report_str = response.choices[0].message.content.strip()
        # JSON íŒŒì‹± ë¡œì§ì€ ê¸°ì¡´ ì½”ë“œë¥¼ ì¬ì‚¬ìš©
        match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", report_str)
        if match:
            report_str = match.group(1)
        
        return json.loads(report_str)
        
    except Exception as e:
        print(f"âš ï¸ [ìš”ì•½ ë³´ê³ ì„œ] GPT í˜¸ì¶œ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {"error": str(e)}


# --- [ë³´ê³ ì„œ 2] ìƒì„¸ ì†ŒìŠ¤ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜ ---
def generate_detailed_sources_report(
    query: Dict[str, Any],
    web_docs: List[Dict], 
    crawling_docs: List[Dict],        # ğŸ‘ˆ ì •ì  DB (ê¸°ì¡´ db_docs)
    user_docs: List[Dict]    # ğŸ‘ˆ ë¼ì´ë¸Œ DB (ì‹ ê·œ user_db_docs)
) -> Dict[str, Any]:
    """
    ê° ìœ ì‚¬ ì‚¬ë¡€(ì†ŒìŠ¤)ë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ëŠ” ë³´ê³ ì„œ(JSON)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """

    query_str = json.dumps(query, ensure_ascii=False, indent=2)
    
    # ğŸ’¡ ë‘ DB ê²°ê³¼ë¥¼ í•©ì³ì„œ LLMì— ì „ë‹¬
    all_db_docs = crawling_docs + user_docs

    # ì†ŒìŠ¤ ë¬¸ì„œ ë¸”ë¡ êµ¬ì„±
    def make_doc_block(docs: List[Dict], prefix: str) -> str:
        blocks = []
        for i, doc in enumerate(docs):
            block = (
                f"[{prefix}{i+1}] Title: {doc.get('title', 'N/A')}\n"
                f"Content: {doc.get('content') or doc.get('snippet', 'N/A')}\n"
                f"Source: {doc.get('source', 'N/A')} (Link: {doc.get('link', 'N/A')})\n"
                f"Keyword: {doc.get('keyword', 'N/A')}\n" 
                f"Team Members: {', '.join(doc.get('team_members', [])) or 'N/A'}\n"
                f"Date: {doc.get('date') or doc.get('updatedAt', 'N/A')}\n"
                f"Score: {doc.get('score', 'N/A')}"
            )
            blocks.append(block)
        return "\n\n".join(blocks)

    # ëª¨ë“  ë‚´ë¶€ DB ê²°ê³¼ë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ DB ë¸”ë¡ìœ¼ë¡œ LLMì— ì „ë‹¬
    all_db_docs = crawling_docs + user_docs
    web_block = make_doc_block(web_docs, "WEB")
    db_block = make_doc_block(all_db_docs, "DB")
    
    # --- í”„ë¡¬í”„íŠ¸ êµ¬ì„± (query_str ì‚¬ìš©) ---
    prompt = f"""
ë‹¹ì‹ ì€ ì°½ì—… ì•„ì´ë””ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤:
"{query}"

ì•„ë˜ëŠ” ìœ„ ì•„ì´ë””ì–´ì™€ ê´€ë ¨ëœ ìœ ì‚¬ ìë£Œ ëª©ë¡ì…ë‹ˆë‹¤. ê° ìë£Œë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¡´ëŒ“ë§ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
---
[ìœ ì‚¬í•œ ì›¹ ë¬¸ì„œ]
{web_block}

[ìœ ì‚¬í•œ ê³µëª¨ì „ ìˆ˜ìƒì‘ ë° ë‚´ë¶€ DB ì‚¬ë¡€]
{db_block}
---

ê° ë¬¸ì„œì— ëŒ€í•´ ë‹¤ìŒ í•­ëª©ì„ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”.

[í‰ê°€ ê¸°ì¤€]
1. `source_type`: "web" ë˜ëŠ” "internal_db"ë¡œ ì§€ì •
2. `title`: ì›ë³¸ ë¬¸ì„œ ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
3. `summary`: ë¬¸ì„œ ìš”ì•½
4. `score`: ìœ ì‚¬ë„ ì ìˆ˜ (ì •ìˆ˜ ë˜ëŠ” ì‹¤ìˆ˜)
5. insight: ì´ ìœ ì‚¬ ì‚¬ë¡€ì˜ 'Keyword' í•„ë“œ, 'Team Members' í•„ë“œ, 'Link' í•„ë“œ, 'Source' í•„ë“œë¥¼ í™œìš©í•˜ì—¬, í˜„ì¬ ì•„ì´ë””ì–´ì™€ ì´ ì‚¬ë¡€ì˜ ì°¨ì´ì  ë˜ëŠ” ì°¸ê³ í•  ë§Œí•œ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ ì¡´ëŒ“ë§ë¡œ ì„œìˆ í•˜ì„¸ìš”. InsightëŠ” ì•„ë˜ [Insight ì¶œë ¥ êµ¬ì¡° ì˜ˆì‹œ]ì— ë”°ë¼ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
[Insight ì¶œë ¥ êµ¬ì¡° ì˜ˆì‹œ] <ê¸°ì¡´ ì•„ì´ë””ì–´> ê°œë³„ ì¼ì • ê´€ë¦¬ ì¤‘ì‹¬ ë‹¨ì¼ ì‚¬ìš©ì ìœ„ì£¼ ì„¤ê³„ <ë‚´ ì•„ì´ë””ì–´> íŒ€í”Œ ì¼ì • ìë™ ì¶”ì²œ: ìº˜ë¦°ë” ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ íŒ€ ì „ì²´ ì¼ì • ìµœì  ì‹œê°„ëŒ€ ìë™ ì œì•ˆ í”„ë¼ì´ë²„ì‹œ ì¡´ì¤‘ ì¼ì • ê³µìœ : ì—…ë¬´ëª…ì€ ìˆ¨ê¸°ê³  ìƒíƒœ(ë°”ì¨/ì—¬ìœ )ë§Œ í‘œì‹œ â†’ ë¶€ë‹´ ìµœì†Œí™” ì§‘ì¤‘ ëª¨ë“œ ì—°ê³„: ì¼ì • ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ â€˜ë°©í•´ê¸ˆì§€ ëª¨ë“œ + ëª¨ê°ì‘ ì§‘ì¤‘ë°© ì°¸ì—¬â€™ ì—°ë™ í¬ë¡œìŠ¤ íˆ´ ì—°ê²°: Notion/Trelloì™€ ì–‘ë°©í–¥ ì‹±í¬ â†’ í”„ë¡œì íŠ¸ ê´€ë¦¬ + ì¼ì •ê´€ë¦¬ í†µí•©

[ì¶œë ¥ JSON í˜•ì‹]

```json

{{
  "detailed_results": [
    {{
      "source_type": "internal_db",
      "title": "ë‚´ë¶€ ê³µëª¨ì „: ìŠ¤ë§ˆíŠ¸ ë¬¼ë¥˜ ìµœì í™” ì‹œìŠ¤í…œ",
      "summary": "AI ê¸°ë°˜ ë¬¼ë¥˜ ì°½ê³  ê´€ë¦¬ ë° ê²½ë¡œ ìµœì í™” ì†”ë£¨ì…˜ì„ í†µí•´ ë°°ì†¡ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ìš´ì˜ ë¹„ìš©ì„ ì ˆê°í•˜ëŠ” í”„ë¡œì íŠ¸. ì£¼ìš” ê¸°ìˆ ì€ ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ ëª¨ë¸ì…ë‹ˆë‹¤.",
      "score": 85.5,
      "insight": "<ê¸°ì¡´ ì•„ì´ë””ì–´>\n**ë¬¼ë¥˜ ì°½ê³ ** ëŒ€ìƒ B2B ì†”ë£¨ì…˜\nì¬ê³  ë° ê²½ë¡œ ì˜ˆì¸¡ ìµœì í™” ì¤‘ì‹¬\n<ë‚´ ì•„ì´ë””ì–´>\n**ë„ì‹¬ ë¼ìŠ¤íŠ¸ ë§ˆì¼** ë°°ì†¡ íš¨ìœ¨í™”: ì†Œê·œëª¨/ë‹¤ì¤‘ ë°°ì†¡ì§€ ìµœì  ê²½ë¡œ ì‹¤ì‹œê°„ ì œì•ˆ\nììœ¨ ë“œë¡  ë°°ì†¡ í†µí•© ëª¨ë¸: íŠ¹ì • ì§€ì—­(ìº í¼ìŠ¤/ì‹ ë„ì‹œ) ë‚´ **ë“œë¡  ì—°ê³„** íŒŒì¼ëŸ¿ êµ¬ì¶•\nìˆ˜ìµ ëª¨ë¸ ì°¸ê³ : ì´ˆê¸° ì„¤ì¹˜ë¹„ ì—†ëŠ” **êµ¬ë…í˜• SaaS** ë° ì„±ëŠ¥ ê°œì„  ì‹œ ì„±ê³¼ ê³µìœ  ëª¨ë¸ ë„ì…\n(Keyword: AI, ë¬¼ë¥˜ ìµœì í™”, ë”¥ëŸ¬ë‹, Team Members: 5ëª…, Source: ë‚´ë¶€DB)"
    }},
    {{
      "source_type": "web",
      "title": "2024ë…„ ì „êµ­ ëŒ€í•™ìƒ ì•„ì´ë””ì–´ ê²½ì§„ëŒ€íšŒ ìµœìš°ìˆ˜ìƒ: ì§€ì†ê°€ëŠ¥í•œ íê¸°ë¬¼ ê´€ë¦¬ í”Œë«í¼",
      "summary": "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ íê¸°ë¬¼ ë°°ì¶œë¶€í„° ì²˜ë¦¬ê¹Œì§€ ì „ ê³¼ì •ì„ íˆ¬ëª…í•˜ê²Œ ê¸°ë¡í•˜ê³ , ì¸ì„¼í‹°ë¸Œë¥¼ ì œê³µí•˜ì—¬ ì‹œë¯¼ ì°¸ì—¬ë¥¼ ìœ ë„í•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤.",
      "score": 72,
      "insight": "<ê¸°ì¡´ ì•„ì´ë””ì–´>\n**ë¸”ë¡ì²´ì¸** ê¸°ë°˜ì˜ **í™˜ê²½** ë¬¸ì œ í•´ê²° í”Œë«í¼\níê¸°ë¬¼ íˆ¬ëª…ì„± ë° ì‹œë¯¼ ì¸ì„¼í‹°ë¸Œ ì œê³µ ì¤‘ì‹¬\n<ë‚´ ì•„ì´ë””ì–´>\n**ëŒ€í•™ ìƒí™œ** í™˜ê²½ íŠ¹í™”: êµë‚´ ê³µìš© ë¬¼í’ˆ/íê¸°ë¬¼ ìˆœí™˜ ë° ì¤‘ê³  ê±°ë˜ í†µí•© ê´€ë¦¬\n**ì¸ì„¼í‹°ë¸Œ ëª¨ë¸** í™•ì¥: í† í°ì„ êµë‚´ ì¹´í˜, ë„ì„œ ëŒ€ì—¬ í• ì¸ ë“± **ì‹¤ì§ˆì  ë³´ìƒ**ê³¼ ì—°ê³„\nê¸°ìˆ  ì°¸ê³ : ë¸”ë¡ì²´ì¸ ëŒ€ì‹  AI ê¸°ë°˜ **íƒ„ì†Œ ë°œìêµ­** ì¸¡ì •ìœ¼ë¡œ ê¸°ìˆ  ì°¨ë³„í™” ëª¨ìƒ‰\n(Keyword: ë¸”ë¡ì²´ì¸, í™˜ê²½, ì¸ì„¼í‹°ë¸Œ, Team Members: 4ëª…, Source: ê²½ì§„ëŒ€íšŒ ê³µì‹ í™ˆí˜ì´ì§€)"
    }}
  ]
}}
```
ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ê°ì²´ë§Œ ìƒì„±í•˜ì„¸ìš”. ì´ì œ í‰ê°€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.
"""
    
    # LLMì´ ë¶„ì„í•  ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    results_list = web_docs + all_db_docs # LLMì´ ë¶„ì„í•œ í›„ ì´ ìˆœì„œëŒ€ë¡œ JSONì„ ë°˜í™˜í•´ì•¼ í•¨.

    try:
        response = client.chat.completions.create(
            model="gpt-5-nano", # ìµœì‹  ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
            messages=[{"role": "user", "content": prompt.strip()}]
        )
        report_str = response.choices[0].message.content.strip()

        # ğŸ’¡ [ìˆ˜ì •] JSON íŒŒì‹± ì‹œ ìµœì¢… ë°˜í™˜ í˜•ì‹ì¸ {"detailed_results": [...]}ì— ë§ì¶¤
        match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", report_str)
        if match:
            report_str = match.group(1)
        
        parsed_json = json.loads(report_str)
        
        return {"query": query, "detailed_results": parsed_json.get("detailed_results", []), "raw_report": parsed_json}

    except Exception as e:
        print(f"âš ï¸ [ìƒì„¸ ë³´ê³ ì„œ] GPT í˜¸ì¶œ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {"query": query, "detailed_results": [], "error": str(e)}



if __name__ == "__main__":    
    # 1. ì‚¬ìš©ì ì•„ì´ë””ì–´ ì¿¼ë¦¬
    test_query = {
        "title": "ë‚´ ë§ˆìŒì˜ ì¹œêµ¬, AI ë©˜íƒˆ ì¼€ì–´ ì±—ë´‡ 'MindMate'",
        "content": "ê°œì¸ì˜ ê°ì • ìƒíƒœ, ê³¼ê±° ëŒ€í™” ì´ë ¥ ë° ì„±í–¥ì„ ë”¥ëŸ¬ë‹ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ë§ì¶¤í˜• ìƒë‹´ ìŠ¤í¬ë¦½íŠ¸ì™€ ì •ì„œì  ì§€ì›ì„ ì œê³µí•˜ëŠ” 24ì‹œê°„ ë©˜íƒˆ ì¼€ì–´ ì±—ë´‡ ì„œë¹„ìŠ¤. ì‚¬ìš©ìì—ê²Œ ì‹¬ë¦¬ ì „ë¬¸ê°€ ì—°ê²° ì˜µì…˜ë„ ì œê³µ."
    }

    # 2. ì›¹ ê²€ìƒ‰ ê²°ê³¼
    test_web_docs = [
        {
            "title": "BetterHelp, ì˜¨ë¼ì¸ ì‹¬ë¦¬ ìƒë‹´ ì‹œì¥ ì„ ë‘ì£¼ì",
            "snippet": "ìœ ë£Œ êµ¬ë… ê¸°ë°˜ìœ¼ë¡œ ì‹¬ë¦¬ ì „ë¬¸ê°€ì™€ ë§¤ì¹­í•˜ëŠ” í”Œë«í¼. AIëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.",
            "source": "TechCrunch",
            "link": "http://example.com/betterhelp"
        },
        {
            "title": "Wysa: AI ê¸°ë°˜ ë©˜íƒˆ í—¬ìŠ¤ ì±—ë´‡, ì‚¬ìš©ì 500ë§Œ ëŒíŒŒ",
            "snippet": "ì´ˆê¸° ì‹¬ë¦¬ ì§€ì›ì— AIë¥¼ í™œìš©í•˜ë©°, í•„ìš” ì‹œ ì¸ê°„ ì „ë¬¸ê°€ì™€ ì—°ê²°í•˜ëŠ” ëª¨ë¸ ì‚¬ìš©.",
            "source": "Forbes",
            "link": "http://example.com/wysa"
        }
    ]

    # 3. í¬ë¡¤ë§ DB (ì •ì /ê³µëª¨ì „ ë°ì´í„°)
    test_crawling_docs = [
        {
            "title": "2023 ê³¼í•™ê¸°ìˆ  ê³µëª¨ì „ ëŒ€ìƒ: ë”¥ëŸ¬ë‹ ê¸°ë°˜ ìš°ìš¸ì¦ ì§„ë‹¨ ì•±",
            "content": "ìŒì„± ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìš°ìš¸ì¦ ì´ˆê¸° ì§•í›„ë¥¼ ì§„ë‹¨í•˜ëŠ” ì†”ë£¨ì…˜. ì±—ë´‡ ê¸°ëŠ¥ ì—†ìŒ.",
            "keyword": "ë”¥ëŸ¬ë‹, ìš°ìš¸ì¦ ì§„ë‹¨, ìŒì„± ë¶„ì„",
            "team_members": ["ê¹€", "ì´", "ë°•"],
            "source": "ê³µëª¨ì „DB",
            "link": "http://internal.db/crawling1",
            "score": 88
        }
    ]

    # 4. ìœ ì € DB (ë¼ì´ë¸Œ ë°ì´í„°)
    test_user_docs = [
        {
            "title": "í•™êµ ì°½ì—…íŒ€ í”„ë¡œì íŠ¸: ì²­ì†Œë…„ ìµëª… ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ì±„íŒ…ë°©",
            "content": "ë˜ë˜ ì²­ì†Œë…„ë“¤ì´ ìµëª…ìœ¼ë¡œ ê³ ë¯¼ì„ ë‚˜ëˆ„ëŠ” ì»¤ë®¤ë‹ˆí‹° í”Œë«í¼. AI ê¸°ëŠ¥ì€ ë‹¨ìˆœ í‚¤ì›Œë“œ í•„í„°ë§ì—ë§Œ ì‚¬ìš©.",
            "keyword": "ì»¤ë®¤ë‹ˆí‹°, ìµëª…, ì²­ì†Œë…„",
            "team_members": ["ìµœ", "ì •"],
            "source": "UserDB_A",
            "link": "http://internal.db/user1",
            "score": 65
        }
    ]

    
    print("--- ğŸ§  ì•„ì´ë””ì–´ ë¶„ì„ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ---")
    # ì´ ìœ ì‚¬ ê±´ìˆ˜ëŠ” ì›¹ ë¬¸ì„œ ê°œìˆ˜ë¥¼ í¬í•¨í•˜ì—¬ ëŒ€ëµì ìœ¼ë¡œ 100ê±´ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
    APPROX_WEB_COUNT = 10 

    # 1. ìš”ì•½ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("[1] ìš”ì•½ ë³´ê³ ì„œ (Summary Report) ìƒì„± í…ŒìŠ¤íŠ¸")
    try:
        summary_report = generate_summary_report(
            query=test_query,
            web_docs=test_web_docs,
            crawling_docs=test_crawling_docs,
            user_docs=test_user_docs,
            approx_similar_count=APPROX_WEB_COUNT
        )
        print("âœ… ìš”ì•½ ë³´ê³ ì„œ ê²°ê³¼:")
        print(json.dumps(summary_report, ensure_ascii=False, indent=4))
    except Exception as e:
        print(f"âŒ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print("\n" + "="*50)

    # 2. ìƒì„¸ ì†ŒìŠ¤ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
    print("[2] ìƒì„¸ ì†ŒìŠ¤ ë³´ê³ ì„œ (Detailed Sources Report) ìƒì„± í…ŒìŠ¤íŠ¸")
    try:
        detailed_report = generate_detailed_sources_report(
            query=test_query,
            web_docs=test_web_docs,
            crawling_docs=test_crawling_docs,
            user_docs=test_user_docs
        )
        print("âœ… ìƒì„¸ ì†ŒìŠ¤ ë³´ê³ ì„œ ê²°ê³¼ (ìƒìœ„ 1ê°œë§Œ ì¶œë ¥):")
        # ê²°ê³¼ë¥¼ ê¹”ë”í•˜ê²Œ ë³´ê¸° ìœ„í•´ ì²« ë²ˆì§¸ í•­ëª©ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
        if detailed_report.get("detailed_results"):
            print(json.dumps(detailed_report.get("detailed_results")[0], ensure_ascii=False, indent=4) + "...")
        else:
            print(json.dumps(detailed_report, ensure_ascii=False, indent=4))
            
    except Exception as e:
        print(f"âŒ ìƒì„¸ ì†ŒìŠ¤ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    print("\n--- ğŸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ---")