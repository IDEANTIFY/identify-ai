import os
import faiss
import pickle
import pandas as pd
import torch
import sys
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any


class CrawlingdbFaissSearchEngine:# ğŸ’¡ í´ë˜ìŠ¤ëª… ë³€ê²½: FaissSearchEngine -> CrawlingdbFaissSearchEngine
    """
    SentenceTransformerì™€ FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ CSV ë°ì´í„°ì— ëŒ€í•œ ì˜ë¯¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„.
    ì¸ë±ìŠ¤ êµ¬ì¶•, ë¡œë”©, ê²€ìƒ‰ ê¸°ëŠ¥ì„ í´ë˜ìŠ¤ë¡œ ìº¡ìŠí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    """
    def __init__(self, model_path: str):
        """
        ì—”ì§„ ì´ˆê¸°í™” ì‹œ SentenceTransformer ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        GPU ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë©´ ìë™ìœ¼ë¡œ GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"'{self.device}' ì¥ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤.", flush = True)
        self.model = SentenceTransformer(model_path, device=self.device)
        self.index = None
        self.metadata = []

    def build_index(self, csv_path: str, index_path: str, meta_path: str):

        """
        CSV íŒŒì¼ë¡œë¶€í„° FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            csv_path (str): ì›ë³¸ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ.
            index_path (str): ìƒì„±ëœ FAISS ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•  ê²½ë¡œ.
            meta_path (str): ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°(pickle)ë¥¼ ì €ì¥í•  ê²½ë¡œ.
        """
        print(f"'{csv_path}' íŒŒì¼ë¡œë¶€í„° ì¸ë±ìŠ¤ êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤...", flush = True)
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        df = pd.read_csv(csv_path)
        ### ğŸ’¡ ìˆ˜ì • : ì»¬ëŸ¼ ì¶”ê°€
        df.dropna(subset=['title', 'content', 'detail_url', 'upper_keyword', 'lower_keyword', 'producer'], inplace=True)        
        df['content_length'] = df['content'].apply(lambda x: len(str(x).split()))
        df = df[df['content_length'] > 10].reset_index(drop=True)
        print(f"ì „ì²˜ë¦¬ í›„ {len(df)}ê°œì˜ ìœ íš¨í•œ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.", flush = True)

        # 2. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        print("í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)", flush = True)
        ### ğŸ’¡ ìˆ˜ì • : ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì¡°í•©: title + content + upper_keyword + lower_keyword
        # texts = (df['title'].fillna('') + " " + df['content'].fillna('')).tolist()
        texts = (
            df['title'].fillna('') + " " + 
            df['content'].fillna('') + " " + 
            df['etc'].fillna('') + " " +   ### ğŸ’¡ ì¶”ê°€
            df['upper_keyword'].fillna('') + " " +  
            df['lower_keyword'].fillna('')  
        ).tolist()

        embeddings = self.model.encode(
            texts, 
            batch_size=128, # GPU ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ
            convert_to_numpy=True, 
            normalize_embeddings=True,
            show_progress_bar=True
        )

        # 3. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ (IndexFlatIP: ë‚´ì  ê¸°ë°˜ ê±°ë¦¬ ê³„ì‚°)
        print("FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤...", flush = True)
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)
        faiss.write_index(index, index_path)

        # 4. ë©”íƒ€ë°ì´í„° ì €ì¥
        print("ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤...", flush = True)
        ### ğŸ’¡ ìˆ˜ì • : ë©”íƒ€ë°ì´í„° í•„ë“œ ëª©ë¡ ìˆ˜ì • -> ìƒí•˜ìœ„ í‚¤ì›Œë“œ(), ì¶œì²˜(detail_url, ë©¤ë²„(producer) ì¶”ê°€
        # metadata = df[['title', 'content', 'date', 'detail_url']].fillna('').to_dict(orient='records')
        metadata_cols = ['title', 'content', 'date', 'detail_url', 'upper_keyword', 'lower_keyword', 'producer']        
        metadata = df[metadata_cols].fillna('').to_dict(orient='records')

        with open(meta_path, 'wb') as f:
            pickle.dump(metadata, f)
            
        print(f"âœ… ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ('{index_path}', '{meta_path}')", flush = True)
        
        # ìƒì„±ëœ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë‚´ë¶€ì— ë¡œë“œ
        self.index = index
        self.metadata = metadata
        
        # ğŸ’¡ ìˆ˜ì •: ì¸ë±ìŠ¤ êµ¬ì¶• ì„±ê³µ ì‹œ True ë°˜í™˜ ì¶”ê°€
        return True

    def load_index(self, index_path: str, meta_path: str):
        """
        ë¯¸ë¦¬ êµ¬ì¶•ëœ FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            index_path (str): FAISS ì¸ë±ìŠ¤ íŒŒì¼(.index) ê²½ë¡œ.
            meta_path (str): ë©”íƒ€ë°ì´í„° íŒŒì¼(.pkl) ê²½ë¡œ.
        """
        if not (os.path.exists(index_path) and os.path.exists(meta_path)):
            # raise FileNotFoundError(f"ì¸ë±ìŠ¤ íŒŒì¼ '{index_path}' ë˜ëŠ” ë©”íƒ€ë°ì´í„° íŒŒì¼ '{meta_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        print(f"'{index_path}' ë° '{meta_path}'ì—ì„œ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...", flush = True)
        self.index = faiss.read_index(index_path)
        with open(meta_path, 'rb') as f:
            self.metadata = pickle.load(f)
        print("âœ… ë¡œë”©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", flush = True)

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ë¡œë“œëœ ì¸ë±ìŠ¤ì—ì„œ ì£¼ì–´ì§„ ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        Args:
            query (str): ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬.
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ì˜ ìˆ˜.

        Returns:
            List[Dict[str, Any]]: scoreê°€ í¬í•¨ëœ ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        if self.index is None:
            raise RuntimeError("ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê¸° ì „ì— 'build_index' ë˜ëŠ” 'load_index'ë¥¼ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")
            
        # 1. ì¿¼ë¦¬ ì„ë² ë”© ë° ì •ê·œí™”
        query_vec = self.model.encode([query], convert_to_numpy=True, normalize_embeddings=True)

        # 2. FAISS ê²€ìƒ‰ (D: ê±°ë¦¬, I: ì¸ë±ìŠ¤)
        distances, indices = self.index.search(query_vec, top_k)

        # 3. ë©”íƒ€ë°ì´í„°ì™€ ê²°í•©í•˜ì—¬ ê²°ê³¼ í¬ë§·íŒ…
        # results = []
        # for idx, dist in zip(indices[0], distances[0]):
        #     if idx != -1:  # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ ê²½ìš°
        #         item = self.metadata[idx].copy()
        #         item['score'] = float(dist)
        #         results.append(item)
                
        # return results

        ### ğŸ’¡ ìˆ˜ì • : ìµœì¢… ì¶œë ¥ í¬ë§·ì— í‚¤ì›Œë“œ ì¶”ê°€
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx != -1:
                item = self.metadata[idx].copy()
                link_url = item.get('detail_url', f"/archive/{idx}") 
                
                # link: CSVì˜ detail_url í•„ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                link_url = item.get('detail_url', f"/archive/{idx}") 
                
                # team_members : producer í•„ë“œë¥¼ team_membersë¡œ ë§¤í•‘ ë° í¬ë§·íŒ…
                # producerê°€ ë¬¸ìì—´ì´ë¯€ë¡œ ìµœì¢…ì ìœ¼ë¡œëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ Live Engineê³¼ í†µì¼ì‹œí‚µë‹ˆë‹¤.
                producer_data = item.get('producer', 'ì •ë³´ ì—†ìŒ')
                if isinstance(producer_data, str) and ',' in producer_data:
                    member_names = [name.strip() for name in producer_data.split(',')]
                elif isinstance(producer_data, str) and producer_data:
                    member_names = [producer_data]
                else:
                    member_names = []

                results.append({
                    "score": float(dist),
                    "source": "ì™¸ë¶€",
                    "title": item['title'], 
                    "content": item['content'],
                    "keyword": item['lower_keyword'], # ğŸ‘ˆ í•˜ìœ„_í‚¤ì›Œë“œ ì¶”ê°€
                    "date": item['date'],
                    "link": link_url,
                    "team_members": member_names
                })
                
        return results




if __name__ == '__main__':
    # -----------------------
    # --- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜ ---
    # -----------------------

    # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€: /identify-ai/app_validator/utils/
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    APP_VALIDATOR_DIR = os.path.dirname(CURRENT_DIR)
    PROJECT_ROOT_PATH = os.path.dirname(APP_VALIDATOR_DIR) # ROOTëŠ” /identify-aiì—¬ì•¼ í•©ë‹ˆë‹¤.
    
    DB_CSV_FILE = os.path.join(PROJECT_ROOT_PATH, "dataset", "crawling_db.csv")
    INDEX_FILE = os.path.join(PROJECT_ROOT_PATH, "dataset", "crawling_total.index")
    META_FILE = os.path.join(PROJECT_ROOT_PATH, "dataset", "crawling_total.pkl")

    MODEL_PATH = "jhgan/ko-sroberta-multitask"
    
    # 1. ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
    try:
        engine = CrawlingdbFaissSearchEngine(model_path=MODEL_PATH) 
    except Exception as e:
        print(f"âŒ ì—”ì§„ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    # 2. ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” êµ¬ì¶• ì‹œë„ (ë‹¨ìˆœí™”ëœ í”Œë¡œìš°)
    index_loaded = engine.load_index(index_path=INDEX_FILE, meta_path=META_FILE)
    
    if not index_loaded:
        print("\n[INFO] ì‚¬ì „ êµ¬ì¶•ëœ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. CSVë¡œë¶€í„° ì¸ë±ìŠ¤ êµ¬ì¶•ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        build_success = engine.build_index(csv_path=DB_CSV_FILE, index_path=INDEX_FILE, meta_path=META_FILE)
        
        if not build_success:
             print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
             sys.exit(1)
        else:
             print("âœ… ì¸ë±ìŠ¤ êµ¬ì¶•ì´ ì™„ë£Œë˜ì–´ ê²€ìƒ‰ì„ ì§„í–‰í•©ë‹ˆë‹¤.") # ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥ í›„ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
             # build_index ì„±ê³µ ì‹œ engine.indexì™€ engine.metadataê°€ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

    # 2. ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰
    if engine.index is not None:
        # print("\n[TEST] ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰: 'ê°€ì¡± ì‹¤ì¢… ì…ì–‘ì•„ë¥¼ ì°¾ì•„ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤'")
        print("\n[TEST] ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰: 'ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•œ ì‹¤ì¢…ì ë° í•´ì™¸ ì…ì–‘ì ì°¾ê¸° í”Œë«í¼'")
        # query = "ê°€ì¡± ì‹¤ì¢… ì…ì–‘ì•„ë¥¼ ì°¾ì•„ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤"
        query = "ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•œ ì‹¤ì¢…ì ë° í•´ì™¸ ì…ì–‘ì ì°¾ê¸° í”Œë«í¼"
        results = engine.search(query, top_k=10)
        
        # 3. ê²°ê³¼ ì¶œë ¥
        if results:
            print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê±´):")
            for i, res in enumerate(results):
                print(f"  {i+1}. Score: {res['score']:.4f}, Title: {res['title']}, Keyword: {res['keyword']}")
            
        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")