import os
import faiss
import pandas as pd
import torch
import json
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Tuple

# DB ì ‘ì† ì •ë³´ Mocking
# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” .env ë˜ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²Œ ì¢‹ìŒ
DB_CONFIG = {
    "DB_TYPE": "MySQL",
    "HOST": "mock_host",
    "USER": "mock_user",
    "PASSWORD": "mock_password",
    "DATABASE": "ideantify"
}

# user ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ
# /home/work/Team_AI/identify-ai/app_validator/user_db.json
USER_DB_PATH = os.path.join(
os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
'user_db.json'
)


class UserdbFaissSearchEngine: # ğŸ’¡ í´ë˜ìŠ¤ëª… ë³€ê²½: LiveFaissSearchEngine -> UserdbFaissSearchEngine
    """
    SentenceTransformerì™€ FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ outer_project (ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë˜ëŠ” DB)ì˜ 
    ì•„ì´ë””ì–´ ë°ì´í„°ì— ëŒ€í•œ ì˜ë¯¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„ (Mocking í¬í•¨).
    """
    def __init__(self, model_path: str):
        """
        ì—”ì§„ ì´ˆê¸°í™” ì‹œ SentenceTransformer ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , user_db ë°ì´í„°ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        <ë™ì¼>
        GPU ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë©´ ìë™ìœ¼ë¡œ GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"'{self.device}' ì¥ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤.", flush = True)
        self.model = SentenceTransformer(model_path, device=self.device)
        self.index = None
        self.metadata = []

    def _fetch_all_ideas_from_db(self) -> pd.DataFrame:
        """
        [MOCK] outer_project í…Œì´ë¸”ì—ì„œ ëª¨ë“  ì•„ì´ë””ì–´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì„ Mockingí•©ë‹ˆë‹¤.
        ì‹¤ì œ DB ì ‘ì† ëŒ€ì‹  JSON íŒŒì¼ì„ ì½ì–´ì™€ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """

        # 1. JSON íŒŒì¼ë¡œë¶€í„° ë°ì´í„° ë¡œë“œ
        with open(USER_DB_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"{DB_CONFIG['DATABASE']} DBì— ì ‘ì†í•˜ì—¬ user_db ë°ì´í„°ë¥¼ ì¶”ì¶œ ì¤‘...") # "outer_project DB ì ‘ì†í•˜ì—¬ ë°ì´í„°ë¥¼ ì¶”ì¶œ ì¤‘..."

        # 2. ë¡œë“œëœ JSON ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(data)
        
        # 3. ì¸ë±ìŠ¤ êµ¬ì¶•ì— í•„ìš”í•œ í•µì‹¬ í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        # ğŸ’¡ idì™€ updatedAtì€ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶”ì¶œí•©ë‹ˆë‹¤. (ëª¨ì•„ë³´ê¸° ìƒì„¸ ëª¨ë‹¬ë¡œ ì´ë™ ë“±)
        required_cols = ['id', 'user_id', 'createdAt', 'updatedAt', 'input_title', 'input_details', 'upper_keyword', 'lower_keyword', 'team_members_json']        
        df = df[required_cols]

        print(f"âœ… DB ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ. {len(df)}ê°œì˜ ì•„ì´ë””ì–´ ë¡œë“œë¨.")
        return df


    def update_and_load_index(self):
        """
        DBì—ì„œ ìµœì‹  ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , ë©”ëª¨ë¦¬ ë‚´ì— FAISS ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.
        """
        df = self._fetch_all_ideas_from_db()
        
        if df.empty:
            print("ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ì–´ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            self.index = None
            self.metadata = []
            return

        print("í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤...", flush = True)
        
        # 1. ê²€ìƒ‰ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ì¡°í•©
        texts = (
            df['input_title'].fillna('') + " " + 
            df['input_details'].fillna('') + " " + 
            df['upper_keyword'].fillna('') + " " + 
            df['lower_keyword'].fillna('')
        ).tolist()


        # 2. ì„ë² ë”© ìƒì„± (db_search_utils.py ë¡œì§ ì¬ì‚¬ìš©)
        embeddings = self.model.encode(
            texts, 
            batch_size=128, # GPU ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ
            convert_to_numpy=True, 
            normalize_embeddings=True
        )

        # 3. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë©”ëª¨ë¦¬ ë¡œë“œ
        print("FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤...", flush = True)
        self.index = faiss.IndexFlatIP(embeddings.shape[1]) # (db_search_utils.py ë¡œì§ ì¬ì‚¬ìš©)
        self.index.add(embeddings)  # ì¶”ê°€
        ### (id, createdAt, updatedAt í¬í•¨)
        metadata_cols = ['id', 'createdAt', 'updatedAt', 'input_title', 'input_details', 'upper_keyword', 'lower_keyword', 'team_members_json']        
        self.metadata = df[metadata_cols].fillna('').to_dict(orient='records')
        print(f"âœ… UserDB ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ. (ì•„ì´ë””ì–´ ìˆ˜: {len(self.metadata)})", flush = True)
    

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ë¡œë“œëœ ì¸ë±ìŠ¤ì—ì„œ ì£¼ì–´ì§„ ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        (db_search_utils.pyì˜ search ì½”ë“œì™€ ë™ì¼)

        Args:
            query (str): ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬.
            top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ì˜ ìˆ˜.

        Returns:
            List[Dict[str, Any]]: scoreê°€ í¬í•¨ëœ ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸.
        """
        if self.index is None:
            raise RuntimeError("ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê¸° ì „ì— 'update_and_load_index'ë¥¼ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")
            
        # 1. ì¿¼ë¦¬ ì„ë² ë”© ë° ì •ê·œí™” (db_search_utils.py ë¡œì§ ì¬ì‚¬ìš©)
        query_vec = self.model.encode([query], convert_to_numpy=True, normalize_embeddings=True)

        # 2. FAISS ê²€ìƒ‰ (db_search_utils.py ë¡œì§ ì¬ì‚¬ìš©)
        distances, indices = self.index.search(query_vec, top_k)

        # 3. ë©”íƒ€ë°ì´í„°ì™€ ê²°í•©í•˜ì—¬ ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx != -1:
                item = self.metadata[idx].copy()
                
                # ğŸ’¡ ë©¤ë²„ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (JSON ë¬¸ìì—´ì¼ ê²½ìš° ë¡œë“œ í›„ ì‚¬ìš©)
                members_data = item.get('team_members_json', []) 
                try:
                    # ë°ì´í„°ê°€ ë¬¸ìì—´ì´ë©´ JSONìœ¼ë¡œ ë¡œë“œ
                    if isinstance(members_data, str):
                        members = json.loads(members_data)
                    else:
                        # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ í˜•íƒœ (DataFrameì´ ìë™ ë³€í™˜í•œ ê²½ìš°)
                        members = members_data 
                        member_names = [m.get('name', 'ì´ë¦„ì—†ìŒ') for m in members]
                except (json.JSONDecodeError, AttributeError):
                    member_names = ['ë°ì´í„° ì˜¤ë¥˜']

                # ğŸ’¡ ìµœì¢… ê²°ê³¼ í¬ë§·íŒ…: create_report.pyì˜ ì…ë ¥ ê·œê²©ì„ ë”°ë¦„
                results.append({
                    "score": float(dist),
                    "source": "Iideantify ìœ ì €", #### ì¶œì²˜ ëª…ì‹œ
                    "title": item['input_title'], 
                    "content": item['input_details'], 
                    "keyword": item['lower_keyword'],
                    "date": item['createdAt'],
                    "link": f"/project/{item['id']}", # í´ë¦­í•˜ë©´ ëª¨ì•„ë³´ê¸° UIë¡œ ì´ë™í–ˆìŒ ì¢‹ê² ëŠ”ë° ì–´ì¹´ì§€? í•´ë‹¹ í˜ì´ì§€ ë§í¬ ì–´ì¼€ ë°›ì§€? í”„ë¡ íŠ¸?ë°±ì—”ë“œ? ë¬¸ì˜ í•„ìš”
                    "team_members": member_names
                })
                
        return results



if __name__ == '__main__':
    # -----------------------
    # --- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜ ---
    # -----------------------

    # user ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ
    # /home/work/Team_AI/identify-ai/app_validator/user_db.json
    USER_DB_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
    'user_db.json'
)
    # 1. ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ë° ì¸ë±ìŠ¤ ë¡œë“œ
    MODEL_PATH = "jhgan/ko-sroberta-multitask"
    engine = UserdbFaissSearchEngine(model_path=MODEL_PATH)    
    engine.update_and_load_index()
    
    # 2. ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰
    print("\n[TEST] ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰: 'ê°€ì¡±ì„ ì°¾ì•„ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤'")
    query = "ê°€ì¡±ì„ ì°¾ì•„ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤"
    results = engine.search(query, top_k=3)
    
    # 3. ê²°ê³¼ ì¶œë ¥
    if results:
        print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê±´):")
        for i, res in enumerate(results):
            # ì ìˆ˜ (ìœ ì‚¬ë„)ëŠ” ê°€ì¥ ë†’ì€ ê²ƒì´ 1.0
            print(f"  {i+1}. Score: {res['score']:.4f}, Title: {res['title']}, Keyword: {res['keyword']}")
        print("---")
        print(f"ê°€ì¥ ìœ ì‚¬í•œ í•­ëª© (ID: {results[0]['link'].split('/')[-1]})ì€ ì‹¤ì¢…ì ê´€ë ¨ ì•„ì´ë””ì–´ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.")
    else:
        print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")