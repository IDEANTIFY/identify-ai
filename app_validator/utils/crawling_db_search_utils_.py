import os
import faiss
import pickle
import pandas as pd
import torch
import sys
import uuid
from sentence_transformers import SentenceTransformer
from sentence_transformers import CrossEncoder
from typing import List, Dict, Any


class CrawlingdbFaissSearchEngine:# ğŸ’¡ í´ë˜ìŠ¤ëª… ë³€ê²½: FaissSearchEngine -> CrawlingdbFaissSearchEngine
    """
    SentenceTransformerì™€ FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ CSV ë°ì´í„°ì— ëŒ€í•œ ì˜ë¯¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„.
    ì¸ë±ìŠ¤ êµ¬ì¶•, ë¡œë”©, ê²€ìƒ‰ ê¸°ëŠ¥ì„ í´ë˜ìŠ¤ë¡œ ìº¡ìŠí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    """
    def __init__(self, model_path: str): # ğŸ’¡ reranker_path ì œê±°
        """
        ì—”ì§„ ì´ˆê¸°í™” ì‹œ SentenceTransformer ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        GPU ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë©´ ìë™ìœ¼ë¡œ GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"'{self.device}' ì¥ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤.", flush = True)
        self.model = SentenceTransformer(model_path, device=self.device)
        
        self.index = None
        self.metadata = []

    def build_index(self, csv_path: str, index_path: str, meta_path: str):

        """
        CSV íŒŒì¼ë¡œë¶€í„° FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        â­ ë¬¸ì„œ ì²­í‚¹(ì œëª©+í‚¤ì›Œë“œ, ë‚´ìš©)ì„ í†µí•´ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
        Args:
            csv_path (str): ì›ë³¸ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ.
            index_path (str): ìƒì„±ëœ FAISS ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•  ê²½ë¡œ.
            meta_path (str): ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°(pickle)ë¥¼ ì €ì¥í•  ê²½ë¡œ.
        """
        print(f"'{csv_path}' íŒŒì¼ë¡œë¶€í„° ì¸ë±ìŠ¤ êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤...", flush = True)
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        df = pd.read_csv(csv_path)
        df.dropna(subset=['title', 'content', 'detail_url', 'upper_keyword', 'lower_keyword', 'producer'], inplace=True)        
        df['content_length'] = df['content'].apply(lambda x: len(str(x).split()))
        df = df[df['content_length'] > 10].reset_index(drop=True)
        print(f"ì „ì²˜ë¦¬ í›„ {len(df)}ê°œì˜ ìœ íš¨í•œ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.", flush = True)

        # 2. í…ìŠ¤íŠ¸ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° í™•ì¥
        texts_to_embed = []
        expanded_metadata = []
        metadata_cols = ['title', 'content', 'date', 'detail_url', 'upper_keyword', 'lower_keyword', 'producer']
        
        # ì›ë³¸ ë¬¸ì„œë¥¼ ìˆœíšŒí•˜ë©° ì—¬ëŸ¬ ê°œì˜ 'ì²­í¬'ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        for original_idx, row in df.iterrows():
            base_meta = row[metadata_cols].fillna('').to_dict()
            
            # --- ì²­í¬ ì „ëµ 1: ì œëª©ê³¼ í‚¤ì›Œë“œ ì¡°í•© (ì¿¼ë¦¬ê°€ ì§§ê³  í•µì‹¬ì¼ ë•Œ ìœ ë¦¬) ---
            chunk_title_keyword = (
                base_meta['title'] + " " +
                base_meta['upper_keyword'] + " " +
                base_meta['lower_keyword']
            ).strip()
            
            if chunk_title_keyword:
                texts_to_embed.append(chunk_title_keyword)
                meta_item = base_meta.copy()
                meta_item['document_id'] = str(uuid.uuid4())
                meta_item['chunk_type'] = 'TITLE_KEYWORD'
                meta_item['text'] = chunk_title_keyword
                expanded_metadata.append(meta_item)
                
            # --- ì²­í¬ ì „ëµ 2: ë‚´ìš©ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  ë° ì¸ë±ì‹± (ì¿¼ë¦¬ê°€ Contentì™€ ì¼ì¹˜í•  ë•Œ ê°€ì¥ ì¤‘ìš”) ---
            chunk_content_full = base_meta['content'].strip()
            
            if chunk_content_full:
                # ğŸŒŸ ìˆ˜ì •: ë§ˆì¹¨í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë¶„í•  (í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„í• ì„ ìœ„í•œ ê°„ë‹¨í•œ ë°©ë²•)
                sentences = [s.strip() for s in chunk_content_full.split('.') if s.strip()] 
                
                # ë¬¸ì¥ì„ 1ê°œ ë˜ëŠ” 2ê°œì”© ë¬¶ì–´ ì§§ì€ ë‹¨ë½ ì²­í¬ ìƒì„± (ë¬¸ë§¥ ìœ ì§€ ë° ì ì ˆí•œ ê¸¸ì´ í™•ë³´)
                # 1ê°œì”© ë¬¶ëŠ” ê²ƒì´ ì¿¼ë¦¬ì™€ ì •í™•íˆ ì¼ì¹˜í•  ë•Œ ê°€ì¥ ìœ ë¦¬í•©ë‹ˆë‹¤.
                chunk_size = 1 # ë¬¸ì¥ í•˜ë‚˜ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ!
                sentence_chunks = [' '.join(sentences[i:i + chunk_size]) 
                                   for i in range(0, len(sentences), chunk_size)]

                for i, sentence_chunk in enumerate(sentence_chunks):
                    # ì¿¼ë¦¬ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¬¸ì¥ì´ ì¸ë±ì‹±ë˜ë„ë¡ í•©ë‹ˆë‹¤.
                    if len(sentence_chunk.split()) >= 5: # ë„ˆë¬´ ì§§ì€ ì²­í¬(ë‹¨ì–´ 5ê°œ ë¯¸ë§Œ) ì œì™¸
                        texts_to_embed.append(sentence_chunk)
                        meta_item = base_meta.copy()
                        meta_item['document_id'] = str(uuid.uuid4())
                        # ì´ ì²­í¬ëŠ” ë‚´ìš©ì˜ ì¼ë¶€ì„ì„ ëª…ì‹œ
                        meta_item['chunk_type'] = f'CONTENT_CHUNK_{i+1}' 
                        meta_item['text'] = sentence_chunk # ì„ë² ë”©ì— ì‚¬ìš©ëœ í…ìŠ¤íŠ¸
                        expanded_metadata.append(meta_item)

        print(f"ì²­í‚¹ í›„ ì´ {len(texts_to_embed)}ê°œì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.", flush = True)
        
        # 3. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(
            texts_to_embed,  # â­ ìˆ˜ì • : 'texts'ë¥¼ 'texts_to_embed'ë¡œ ìˆ˜ì •            
            batch_size=128, # GPU ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ
            convert_to_numpy=True, 
            normalize_embeddings=True,
            show_progress_bar=True
        )

        # 3. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ (IndexFlatIP: ë‚´ì  ê¸°ë°˜ ê±°ë¦¬ ê³„ì‚°)
        print("FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤...", flush = True)
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)
        faiss.write_index(index, index_path)

        # 4. ë©”íƒ€ë°ì´í„° ì €ì¥
        print("ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤...", flush = True)
        ### â­ ìˆ˜ì • : í™•ì¥ëœ ë©”íƒ€ë°ì´í„° ì €ì¥
        # metadata_cols = ['title', 'content', 'date', 'detail_url', 'upper_keyword', 'lower_keyword', 'producer']        
        # metadata = df[metadata_cols].fillna('').to_dict(orient='records')
        self.metadata = expanded_metadata # í´ë˜ìŠ¤ ë©¤ë²„ ì—…ë°ì´íŠ¸
        with open(meta_path, 'wb') as f:
            pickle.dump(self.metadata, f)

            
        print(f"âœ… ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ('{index_path}', '{meta_path}')", flush = True)
        
        # ìƒì„±ëœ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë‚´ë¶€ì— ë¡œë“œ
        self.index = index
        
        return True

    def load_index(self, index_path: str, meta_path: str):
        """
        ë¯¸ë¦¬ êµ¬ì¶•ëœ FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            index_path (str): FAISS ì¸ë±ìŠ¤ íŒŒì¼(.index) ê²½ë¡œ.
            meta_path (str): ë©”íƒ€ë°ì´í„° íŒŒì¼(.pkl) ê²½ë¡œ.
        """
        if not (os.path.exists(index_path) and os.path.exists(meta_path)):
            # raise FileNotFoundError(f"ì¸ë±ìŠ¤ íŒŒì¼ '{index_path}' ë˜ëŠ” ë©”íƒ€ë°ì´í„° íŒŒì¼ '{meta_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        print(f"'{index_path}' ë° '{meta_path}'ì—ì„œ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...", flush = True)
        self.index = faiss.read_index(index_path)
        with open(meta_path, 'rb') as f:
            self.metadata = pickle.load(f)
        print("âœ… ë¡œë”©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", flush = True)

    # ğŸ’¡ search í•¨ìˆ˜: ì¬ìˆœìœ„í™” ë¡œì§ ì œê±°, ì²­í‚¹ ê²°ê³¼ í¬ë§·íŒ…ë§Œ ìœ ì§€
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Bi-Encoderì™€ ì²­í‚¹ëœ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if self.index is None:
            raise RuntimeError("ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê¸° ì „ì— 'build_index' ë˜ëŠ” 'load_index'ë¥¼ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")
            
        # 1. ì¿¼ë¦¬ ì„ë² ë”© ë° ì •ê·œí™”
        query_vec = self.model.encode([query], convert_to_numpy=True, normalize_embeddings=True)

        # 2. FAISS ê²€ìƒ‰
        distances, indices = self.index.search(query_vec, top_k)

        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx != -1:
                item = self.metadata[idx].copy()
                link_url = item.get('detail_url', f"/archive/{idx}") 
                
                producer_data = item.get('producer', 'ì •ë³´ ì—†ìŒ')
                member_names = [name.strip() for name in producer_data.split(',')] if isinstance(producer_data, str) and producer_data else []

                results.append({
                    "score": float(dist),
                    "source": "ì™¸ë¶€",
                    "title": item['title'], 
                    "content": item['text'],  # â­ ì²­í¬ëœ ë‚´ìš© ë°˜í™˜
                    "chunk_type": item['chunk_type'], # â­ ì²­í¬ ìœ í˜• ë°˜í™˜
                    "keyword": item['lower_keyword'],
                    "date": item['date'],
                    "link": link_url,
                    "team_members": member_names
                })
                
        return results


if __name__ == '__main__':
    # -----------------------
    # --- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜ ---
    # -----------------------

    # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€: /identify-ai/app_validator/utils/
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    APP_VALIDATOR_DIR = os.path.dirname(CURRENT_DIR)
    PROJECT_ROOT_PATH = os.path.dirname(APP_VALIDATOR_DIR) # ROOTëŠ” /identify-aiì—¬ì•¼ í•©ë‹ˆë‹¤.
    
    DB_CSV_FILE = os.path.join(PROJECT_ROOT_PATH, "dataset","crawling", "crawling_db.csv")
    INDEX_FILE = os.path.join(PROJECT_ROOT_PATH, "dataset","crawling",  "crawling_total.index")
    META_FILE = os.path.join(PROJECT_ROOT_PATH, "dataset","crawling",  "crawling_total.pkl")

    MODEL_PATH = "jhgan/ko-sroberta-multitask"
    
    # 1. ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
    try:
        engine = CrawlingdbFaissSearchEngine(model_path=MODEL_PATH) 
    except Exception as e:
        print(f"âŒ ì—”ì§„ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    # 2. ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” êµ¬ì¶• ì‹œë„ (ë‹¨ìˆœí™”ëœ í”Œë¡œìš°)
    index_loaded = engine.load_index(index_path=INDEX_FILE, meta_path=META_FILE)
    
    if not index_loaded:
        print("\n[INFO] ì‚¬ì „ êµ¬ì¶•ëœ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. CSVë¡œë¶€í„° ì¸ë±ìŠ¤ êµ¬ì¶•ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        build_success = engine.build_index(csv_path=DB_CSV_FILE, index_path=INDEX_FILE, meta_path=META_FILE)
        
        if not build_success:
             print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
             sys.exit(1)
        else:
             print("âœ… ì¸ë±ìŠ¤ êµ¬ì¶•ì´ ì™„ë£Œë˜ì–´ ê²€ìƒ‰ì„ ì§„í–‰í•©ë‹ˆë‹¤.") # ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥ í›„ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
             # build_index ì„±ê³µ ì‹œ engine.indexì™€ engine.metadataê°€ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

    # 2. ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰
    if engine.index is not None:
        # print("\n[TEST] ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰: 'ê°€ì¡± ì‹¤ì¢… ì…ì–‘ì•„ë¥¼ ì°¾ì•„ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤'")
        print("\n[TEST] ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰: 'ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•œ ì‹¤ì¢…ì ë° í•´ì™¸ ì…ì–‘ì ì°¾ê¸° í”Œë«í¼'")
        # query = "ê°€ì¡± ì‹¤ì¢… ì…ì–‘ì•„ë¥¼ ì°¾ì•„ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤"
        query = "ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•œ ì‹¤ì¢…ì ë° í•´ì™¸ ì…ì–‘ì ì°¾ê¸° í”Œë«í¼"
        results = engine.search(query, top_k=10)
        
        # 3. ê²°ê³¼ ì¶œë ¥
        if results:
            print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê±´):")
            for i, res in enumerate(results):
                print(f"  {i+1}. Score: {res['score']:.4f}, Title: {res['title']}, Keyword: {res['keyword']}")
            
        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")