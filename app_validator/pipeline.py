import os
import sys
import json
import torch
from sentence_transformers import SentenceTransformer
import concurrent.futures
import time

# --- âš™ï¸ 1. ëª¨ë“ˆ ì„í¬íŠ¸ ---
# ê° ê¸°ëŠ¥ë³„ë¡œ ë¶„ë¦¬ëœ Python íŒŒì¼ì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from .utils.convert_idea_to_query import *
from .utils.web_search_utils import *
from .utils.db_search_utils import *
from .utils.create_report import *

# --- âœ… 2. ì„¤ì • ë° ì „ì—­ ê°ì²´ ì´ˆê¸°í™” ---

# íŒŒì¼ ë° ëª¨ë¸ ê²½ë¡œ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
MODEL_FOLDER_PATH = "models/ko-sroberta-multitask-local"
INDEX_FILE = "dataset/crawling_total.index"
META_FILE = "dataset/crawling_total.pkl"

def initialize_components():
    if torch.cuda.is_available():      # NVIDIA CUDA GPU í™•ì¸
        device = "cuda"
    elif torch.backends.mps.is_available():  # Apple Metal GPU (MPS) í™•ì¸
        device = "mps"
    else:                               # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ CPU ì‚¬ìš©
        device = "cpu"

    # Sentence Transformer ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ)
    print(f"Sentence Transformer ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤... (Device: {device})", flush = True)
    model = SentenceTransformer(MODEL_FOLDER_PATH, device=device)
    
    # ë‚´ë¶€ DB ê²€ìƒ‰ ì—”ì§„(FAISS) ë¡œë”©
    print("ë‚´ë¶€ DB ì¸ë±ìŠ¤ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...", flush = True)
    db_search_engine = FaissSearchEngine(model_path=MODEL_FOLDER_PATH)
    if not (os.path.exists(INDEX_FILE) and os.path.exists(META_FILE)):
        print(f"âŒ [ì˜¤ë¥˜] DB ì¸ë±ìŠ¤ íŒŒì¼({INDEX_FILE}) ë˜ëŠ” ë©”íƒ€ íŒŒì¼({META_FILE})ì´ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr, flush = True)
        sys.exit(1)
    db_search_engine.load_index(index_path=INDEX_FILE, meta_path=META_FILE)
    
    print("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.", flush = True)
    print("=" * 60, flush = True)
    
    return model, db_search_engine

def execute_full_pipeline(structured_idea: dict) -> dict:
    """
    ì‚¬ìš©ì ì•„ì´ë””ì–´ë¥¼ ì…ë ¥ë°›ì•„ ì „ì²´ ê²€ì¦ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³ ,
    ìš”ì•½ ë° ìƒì„¸ ë³´ê³ ì„œë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        user_idea (str): ê²€ì¦ì„ ì›í•˜ëŠ” ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ êµ¬ì¡°.

    Returns:
        dict: 'summary_report'ì™€ 'detailed_report'ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬.
    """
    
    start_time = time.time()

    # 1. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    # ì°¸ê³ : ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” API ì„œë²„ê°€ ì‹œì‘ë  ë•Œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì— ìœ ë¦¬í•©ë‹ˆë‹¤.
    model, db_search_engine = initialize_components()

    # 2. ì•„ì´ë””ì–´ -> ê²€ìƒ‰ ì¿¼ë¦¬ ë³€í™˜
    print("\n[ë‹¨ê³„ 1/4] ì•„ì´ë””ì–´ë¥¼ í•µì‹¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜ ì¤‘...", flush=True)
    search_query = generate_search_query(structured_idea)
    print(f"  ğŸ” ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬: \"{search_query}\"", flush=True)
    
    # 3. ì •ë³´ ê²€ìƒ‰ (ì›¹ & DB ë³‘ë ¬ ì²˜ë¦¬)
    print("\n[ë‹¨ê³„ 2/4] ì›¹ ë° ë‚´ë¶€ DBì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ë³‘ë ¬ë¡œ ê²€ìƒ‰ ì¤‘...", flush=True)
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        # ì›¹ ê²€ìƒ‰ íƒœìŠ¤í¬ ì œì¶œ
        future_web_search = executor.submit(run_web_search_pipeline, search_query, model)
        # DB ê²€ìƒ‰ íƒœìŠ¤í¬ ì œì¶œ
        future_db_search = executor.submit(db_search_engine.search, search_query, 5)
        
        # ê²°ê³¼ ì·¨í•©
        web_search_df = future_web_search.result()
        db_search_results = future_db_search.result()
    
    print(f"  - ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(web_search_df)}ê±´ì˜ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬", flush=True)
    print(f"  - DB ê²€ìƒ‰ ì™„ë£Œ: {len(db_search_results)}ê±´ì˜ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬", flush=True)
    
    # 4. RAGë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
    top_web_docs = web_search_df.head(3).to_dict('records')
    approx_similar_count = len(web_search_df)

    # 5. ë¦¬í¬íŠ¸ ìƒì„± (ìš”ì•½ & ìƒì„¸ ë³‘ë ¬ ì²˜ë¦¬)
    print("\n[ë‹¨ê³„ 3/4] ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ RAG ë¦¬í¬íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ìƒì„± ì¤‘...", flush=True)
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± íƒœìŠ¤í¬ ì œì¶œ
        future_summary = executor.submit(
            generate_summary_report,
            str(structured_idea), top_web_docs, db_search_results, approx_similar_count
        )
        # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± íƒœìŠ¤í¬ ì œì¶œ
        future_detailed = executor.submit(
            generate_detailed_sources_report,
            str(structured_idea), top_web_docs, db_search_results
        )
        
        # ê²°ê³¼ ì·¨í•©
        summary_report = future_summary.result()
        detailed_report = future_detailed.result()
        
    print("  - ì •ëŸ‰ì  ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ.", flush=True)
    print("  - ìƒì„¸ ì†ŒìŠ¤ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ.", flush=True)
    
    # 6. ìµœì¢… ê²°ê³¼ ì·¨í•© ë° ë°˜í™˜
    print("\n[ë‹¨ê³„ 4/4] ìµœì¢… ê²°ê³¼ ì·¨í•© ì™„ë£Œ.", flush=True)
    final_result = {
        "summary_report": summary_report,
        "detailed_report": detailed_report
    }
    
    end_time = time.time()
    print(f"ğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ! (ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)", flush=True)
    
    return final_result

# --- í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‹¤í–‰ ë¸”ë¡ (ì‹¤ì œ ì„œë²„ì—ì„œëŠ” í˜¸ì¶œë˜ì§€ ì•ŠìŒ) ---
'''if __name__ == '__main__':
    test_idea = {
              "ì£¼ìš” ë‚´ìš©": "AI ê¸°ë°˜ ì‹ë‹¨ ë¶„ì„ ë° ë§ì¶¤í˜• ë ˆì‹œí”¼ ì¶”ì²œ ëª¨ë°”ì¼ ì•±",
              "ë„ë©”ì¸": "ê±´ê°• ë° í”¼íŠ¸ë‹ˆìŠ¤, í‘¸ë“œí…Œí¬",
              "ëª©ì ": "ê°œì¸ ë§ì¶¤í˜• ê±´ê°• ê´€ë¦¬ ë° ì‹ìŠµê´€ ê°œì„ ",
              "ì°¨ë³„ì„±": "AIë¥¼ í™œìš©í•œ ìë™ ì‹ë‹¨ ë¶„ì„ ë° ì •ë°€í•œ ë ˆì‹œí”¼ ì¶”ì²œ",
              "í•µì‹¬ ê¸°ìˆ ": "ì¸ê³µì§€ëŠ¥(AI), ë¨¸ì‹ ëŸ¬ë‹, ì´ë¯¸ì§€ ì¸ì‹(ìŒì‹ ì‚¬ì§„ ë¶„ì„)",
              "ì„œë¹„ìŠ¤ ëŒ€ìƒ": "ê±´ê°•ì— ê´€ì‹¬ì´ ë§ì€ ì‚¬ìš©ì, íŠ¹ì • ì‹ë‹¨ì´ í•„ìš”í•œ í™˜ì"
          }
    
    # ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ
    reports = execute_full_pipeline(test_idea)
    
    # ë°˜í™˜ëœ ê²°ê³¼ í™•ì¸
    print("\n" + "="*80)
    print("âœ… í•¨ìˆ˜ê°€ ë°˜í™˜í•œ ìµœì¢… ê²°ê³¼:")
    print("-" * 60)
    print("ğŸ“Š [ìš”ì•½ ë³´ê³ ì„œ]")
    print(json.dumps(reports['summary_report'], indent=2, ensure_ascii=False))
    print("-" * 60)
    print("ğŸ“‘ [ìƒì„¸ ë³´ê³ ì„œ]")
    print(json.dumps(reports['detailed_report'], indent=2, ensure_ascii=False))
    print("="*80)
'''