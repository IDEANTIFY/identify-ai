import os
import re
import json
import torch
import requests
import pandas as pd
from tavily import TavilyClient
from urllib.parse import quote
from typing import Dict, List, Any
import concurrent.futures
from sentence_transformers import SentenceTransformer, util


# --- âš™ï¸ API ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
# ë³´ì•ˆì„ ìœ„í•´ API í‚¤ëŠ” ì½”ë“œì— ì§ì ‘ ì‘ì„±í•˜ëŠ” ëŒ€ì‹  í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
# ì…¸ì—ì„œ export TAVILY_API_KEY="ë‹¹ì‹ ì˜í‚¤" ì™€ ê°™ì´ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY", "")
SERPER_API_KEY = os.environ.get("SERPER_API_KEY", "")
NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET", "")

tavily_client = TavilyClient(api_key=TAVILY_API_KEY)
HTTP_SESSION = requests.Session()  # ì„¸ì…˜ ê°ì²´ ì¬ì‚¬ìš©

SERPER_HEADERS = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}
NAVER_HEADERS = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}

# --- í—¬í¼ í•¨ìˆ˜ ---
def _clean_html(raw_html: str) -> str:
    """HTML íƒœê·¸ë¥¼ ì œê±°í•˜ëŠ” ê°„ë‹¨í•œ ì •ê·œì‹ í•¨ìˆ˜"""
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext

def _contains_korean(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    return bool(re.search(r'[ê°€-í£]', text))

# --- ê°œë³„ API í˜¸ì¶œ í•¨ìˆ˜ ---
def _search_tavily(query: str) -> List[Dict[str, Any]]:
    """Tavily APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        resp = tavily_client.search(
            query=query,
            search_depth="advanced",
            max_results=10,
            country="south korea"
        )
        return resp.get("results", [])
    except Exception as e:
        print(f"[ì˜¤ë¥˜] Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def _search_serper(query: str) -> List[Dict[str, Any]]:
    """Serper APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    url = "https://google.serper.dev/search"
    payload = json.dumps({"q": query})
    try:
        response = requests.post(url, headers=SERPER_HEADERS, data=payload)
        response.raise_for_status()
        return response.json().get("organic", [])
    except Exception as e:
        print(f"[ì˜¤ë¥˜] Serper ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def _search_naver(query: str) -> List[Dict[str, Any]]:
    """Naver Search APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    url = f"https://openapi.naver.com/v1/search/webkr.json?query={quote(query)}&display=10&sort=sim"
    try:
        response = requests.get(url, headers=NAVER_HEADERS)
        response.raise_for_status()
        return response.json().get("items", [])
    except Exception as e:
        print(f"[ì˜¤ë¥˜] Naver ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


def fetch_all_search_results(query: str) -> Dict[str, List[Dict[str, Any]]]:
    """
    Tavily, Serper, Naver APIë¥¼ ë³‘ë ¬ë¡œ í˜¸ì¶œí•˜ì—¬ ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Input:
        - query (str): ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¬¸ìì—´

    Output:
        - Dict[str, List[Dict[str, Any]]]: API ì†ŒìŠ¤ë³„ë¡œ ì •ë¦¬ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
          ì˜ˆ: {'tavily': [...], 'serper': [...], 'naver': [...]}
    """
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ê° API í˜¸ì¶œì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        future_to_api = {
            executor.submit(_search_tavily, query): "tavily",
            executor.submit(_search_serper, query): "serper",
            executor.submit(_search_naver, query): "naver"
        }
        
        results = {"tavily": [], "serper": [], "naver": []}
        for future in concurrent.futures.as_completed(future_to_api):
            api_name = future_to_api[future]
            try:
                results[api_name] = future.result()
            except Exception as exc:
                print(f"[ì˜¤ë¥˜] {api_name} API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
    
    return results

# --- ê²°ê³¼ ì²˜ë¦¬ í•¨ìˆ˜ (Pandas ì—°ì‚° ìµœì í™”) ---
def process_and_merge_results(raw_results: Dict[str, List[Dict[str, Any]]]) -> pd.DataFrame:
    all_dfs = []
    # 1. Tavily ê²°ê³¼ ì²˜ë¦¬ (ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ë³€ê²½)
    if raw_results.get('tavily'):
        df = pd.DataFrame(raw_results['tavily']).rename(columns={'content': 'snippet', 'url': 'link'})
        df["score"] = pd.to_numeric(df.get("score"), errors="coerce").fillna(0)
        if not df.empty and df["score"].max() > 0:
            adaptive_thr = max(0.3, df["score"].max() * 0.6)
            df = df[df["score"] >= adaptive_thr]
        
        # [ìµœì í™”] title ë˜ëŠ” snippetì— í•œêµ­ì–´ê°€ í¬í•¨ëœ í–‰ì„ í•„í„°ë§
        korean_pattern = re.compile(r'[ê°€-í£]')
        df = df[
            df['title'].astype(str).str.contains(korean_pattern) |
            df['snippet'].astype(str).str.contains(korean_pattern)
        ]
        df['api_source'] = 'tavily'
        all_dfs.append(df[['title', 'snippet', 'link', 'api_source']])
    
    # (Serper, Naver ì²˜ë¦¬ëŠ” ê¸°ì¡´ ë¡œì§ì´ íš¨ìœ¨ì ì´ë¯€ë¡œ ìœ ì§€)
    if raw_results.get('serper'):
        df = pd.DataFrame(raw_results['serper'])
        df['snippet'] = df.get('snippet', '') + " " + df.get('description', '')
        df['snippet'] = df['snippet'].str.strip()
        df = df[~df['snippet'].str.contains('Missing', na=False)]
        df['api_source'] = 'serper'
        all_dfs.append(df[['title', 'snippet', 'link', 'api_source']])
    if raw_results.get('naver'):
        df = pd.DataFrame(raw_results['naver']).rename(columns={'description': 'snippet'})
        df['title'] = df['title'].apply(_clean_html)
        df['snippet'] = df['snippet'].apply(_clean_html)
        df['api_source'] = 'naver'
        all_dfs.append(df[['title', 'snippet', 'link', 'api_source']])

    if not all_dfs:
        return pd.DataFrame()
    return pd.concat(all_dfs, ignore_index=True).drop_duplicates(subset='link', keep='first').reset_index(drop=True)

# --- ìœ ì‚¬ë„ ì¬ì •ë ¬ í•¨ìˆ˜ (GPU ì‚¬ìš© ìµœì í™”) ---
def rerank_results_by_similarity(
    results_df: pd.DataFrame,
    reference_text: str,
    model: SentenceTransformer
) -> pd.DataFrame:
    if results_df.empty:
        return results_df

    # [ìµœì í™”] batch_sizeë¥¼ ì§€ì •í•˜ì—¬ GPU ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©
    snippet_list = results_df['snippet'].fillna("").tolist()
    snippet_embeddings = model.encode(
        snippet_list, 
        batch_size=128,  # GPU ì„±ëŠ¥ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥
        convert_to_tensor=True, 
        normalize_embeddings=True,
        show_progress_bar=True # ì§„í–‰ ìƒí™© í™•ì¸
    )
    
    ref_embedding = model.encode(
        reference_text, 
        convert_to_tensor=True, 
        normalize_embeddings=True
    )
    
    cos_scores = util.cos_sim(ref_embedding, snippet_embeddings)[0]
    
    results_df['similarity_score'] = cos_scores.cpu().tolist()
    return results_df.sort_values(by='similarity_score', ascending=False).reset_index(drop=True)


# --- (fetch_all_search_results, get_top_n_results, run_web_search_pipeline, __main__ ë“± ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼) ---
def fetch_all_search_results(query: str) -> Dict[str, List[Dict[str, Any]]]:
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        future_to_api = {
            executor.submit(_search_tavily, query): "tavily",
            executor.submit(_search_serper, query): "serper",
            executor.submit(_search_naver, query): "naver"
        }
        results = {"tavily": [], "serper": [], "naver": []}
        for future in concurrent.futures.as_completed(future_to_api):
            api_name = future_to_api[future]
            try:
                results[api_name] = future.result()
            except Exception as exc:
                print(f"âš ï¸ [{api_name} API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ] {exc}", file=sys.stderr)
    return results

def get_top_n_results(df: pd.DataFrame, n: int = 3) -> List[Dict[str, Any]]:
    return df.head(n).to_dict('records')

def run_web_search_pipeline(query: str, model: SentenceTransformer) -> pd.DataFrame:
    print(f"ğŸ” 1. '{query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...", flush = True)
    raw_results = fetch_all_search_results(query)
    print(f"ğŸ“Š 2. Tavily: {len(raw_results.get('tavily',[]))}, Serper: {len(raw_results.get('serper',[]))}, Naver: {len(raw_results.get('naver',[]))} ê±´ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.", flush = True)
    merged_df = process_and_merge_results(raw_results)
    print(f"âœ… 3. ì´ {len(merged_df)} ê±´ì˜ ê³ ìœ í•œ ê²€ìƒ‰ ê²°ê³¼ë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.", flush = True)
    print("âœ¨ 4. ì›ë³¸ ì¿¼ë¦¬ì™€ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¬ì •ë ¬í•©ë‹ˆë‹¤...", flush = True)
    reranked_df = rerank_results_by_similarity(merged_df, query, model)
    return reranked_df

# if __name__ == "__main__":
    # search_query = "LLMì„ í™œìš©í•œ ê°œì¸í™” ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶• ì‚¬ë¡€"
    # final_results_df = run_web_search_pipeline(search_query)
    
    # if not final_results_df.empty:
        # pd.set_option("display.max_colwidth", 70)
        # print("\n--- ğŸ† ìµœì¢… ì¬ì •ë ¬ëœ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 10ê°œ) ---", flush = True)
        # print(final_results_df.head(10), flush = True)
        # top_3_list = get_top_n_results(final_results_df, n=3)
        # print("\n--- ğŸ¯ ìƒìœ„ 3ê°œ ê²°ê³¼ (í™œìš© ì˜ˆì‹œ) ---", flush = True)
        # for i, item in enumerate(top_3_list):
            # print(f"[{i+1}]\n  - Title: {item['title']}\n  - Link: {item['link']}\n  - Score: {item['similarity_score']:.4f}\n", flush = True)
    # else:
        # print("\n--- ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ ---", flush = True)