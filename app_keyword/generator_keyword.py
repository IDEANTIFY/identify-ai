import pandas as pd
import json
import os
import re
from openai import OpenAI, APIError
from typing import Optional, Tuple, List, Any, Set, Dict
from ast import literal_eval
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
    
# --- 0. í™˜ê²½ ì„¤ì • ë° API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
# API_KEY_STRING = os.environ.get("OPENAI_API_KEY_ahyun", "")
API_KEY_STRING = os.getenv("OPENAI_API_KEY")
# íŒŒì¼ ë° ëª¨ë¸ ê²½ë¡œ
MODEL_NAME = "gpt-5-nano" # "gpt-4o-500k-ko-5nano"
PROJECT_ROOT_PATH = "/home/work/Team_AI/identify-ai"

DB_CSV_FILE = os.path.join(PROJECT_ROOT_PATH, "dataset", "crawling_db.csv")
ORIGINAL_CSV_FILE = DB_CSV_FILE 
JSON_RULE_FILE = os.path.join(PROJECT_ROOT_PATH, "app_keyword", "category_rules.json")
UNMAPPED_CSV_FILE = os.path.join(PROJECT_ROOT_PATH, "dataset", "unmapped_crawling.csv")
TEXT_COLUMN_NAMES = ['title', 'content', 'etc'] # GPT ì…ë ¥ì— ì‚¬ìš©í•  ì»¬ëŸ¼ ëª©ë¡

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œë¨)
client: Optional[OpenAI] = None
if API_KEY_STRING and API_KEY_STRING != "YOUR_API_KEY_HERE":
    try:
        client = OpenAI(api_key=API_KEY_STRING)
        # print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ.")
    except Exception as e:
        # print(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ (API í˜¸ì¶œ ë¶ˆê°€): {e}")
        client = None
# else:
    # print("âš ï¸ ê²½ê³ : API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ GPT í˜¸ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")


# --- 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ---

def load_simplified_rules(file_path: str) -> dict:
    """JSON íŒŒì¼ì„ ì½ì–´ ë°”ë¡œ ë°˜í™˜í•˜ë©°, íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        # print(f"ê²½ê³ : JSON ê·œì¹™ íŒŒì¼ '{file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return {}
    except Exception as e:
        # print(f"ê²½ê³ : JSON ê·œì¹™ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. ì˜¤ë¥˜: {e}")
        return {}

def extract_existing_keywords(rules: dict) -> Tuple[str, str, Set[str]]:
    """ë¡œë“œëœ ë£° ë”•ì…”ë„ˆë¦¬ì—ì„œ ëª¨ë“  ìƒìœ„ ë° í•˜ìœ„ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ê³¼ Setìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    existing_upper_set = set(rules.keys())
    upper_str_to_reuse = ', '.join(sorted(list(existing_upper_set)))

    existing_lower_set = set()
    for sub_categories in rules.values():
        for key in sub_categories.keys():
             try:
                if key.startswith("['") and key.endswith("']"):
                    parsed_keys = literal_eval(key)
                    existing_lower_set.update(parsed_keys)
                else:
                    existing_lower_set.add(key)
             except (ValueError, SyntaxError):
                existing_lower_set.add(key)

    lower_str_to_avoid = ', '.join(sorted(list(existing_lower_set)))
    
    return upper_str_to_reuse, lower_str_to_avoid, existing_lower_set

def combine_content_columns(row: pd.Series, column_names: List[str]) -> str:
    """title, content, etc ì»¬ëŸ¼ì„ í•©ì³ GPT ì…ë ¥ìš© ë‹¨ì¼ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    parts = []
    
    for col in column_names:
        if col in row and pd.notna(row[col]):
            parts.append(str(row[col]).strip())
        
    return ', '.join(parts).strip()


# --- 2. GPT API í˜¸ì¶œ í•¨ìˆ˜ ---

def generate_keywords(content: str, existing_upper_str: str, lower_str_to_avoid: str) -> Tuple[Optional[str], Optional[str], Optional[List[str]]]:
    """GPT APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì½˜í…ì¸ ì— ëŒ€í•œ ìƒìœ„/í•˜ìœ„ í‚¤ì›Œë“œì™€ ì •ê·œì‹ íŒ¨í„´ì„ ì œì•ˆí•©ë‹ˆë‹¤."""
    if client is None:
        return None, None, None

    cleaned_content = str(content).strip()
    if not cleaned_content or cleaned_content.lower() == 'nan':
        return None, None, None

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê·œì¹™ ê°•ì¡° ë° ì •ê·œì‹ ìƒì„± ì§€ì‹œ)
    system_prompt = (
        "ë‹¹ì‹ ì€ ê¸°ìˆ  ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ í‚¤ì›Œë“œì™€ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ì •ê·œì‹ íŒ¨í„´ì„ ì œì•ˆí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì œê³µëœ 'Content'ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ 'ìƒìœ„ í‚¤ì›Œë“œ', 'í•˜ìœ„ í‚¤ì›Œë“œ', ê·¸ë¦¬ê³  í•˜ìœ„ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ **3~5ê°œì˜ í•µì‹¬ ì •ê·œì‹ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸**ë¥¼ ì œì•ˆí•´ì•¼ í•©ë‹ˆë‹¤. "
        "\n**ê·œì¹™:**\n"
        f"1. ìƒìœ„ í‚¤ì›Œë“œ ì œì•ˆ ì‹œ, **í˜„ì¬ ì‹œì ì˜ ìµœì‹  ëª©ë¡**('{existing_upper_str}') ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì í•©í•œ ê¸°ì¡´ í•­ëª©ì´ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ìƒìœ„ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n"
        f"2. í•˜ìœ„ í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ ìƒˆë¡œìš´ ê²ƒì„ ìƒì„±í•´ì•¼ í•˜ë©°, **ë°©ê¸ˆ ìƒì„±ëœ ê·œì¹™ì„ í¬í•¨í•œ ìµœì‹  ê¸ˆì§€ ëª©ë¡**('{lower_str_to_avoid}')ì— ìˆëŠ” ê²ƒì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "3. ë§Œì•½ ì ì ˆí•œ í•˜ìœ„ í‚¤ì›Œë“œê°€ ì—¬ëŸ¬ ê°œë¼ê³  íŒë‹¨ë˜ë©´, ', ' (íŒŒì´í”„ ìŠ¤í˜ì´ìŠ¤)ë¡œ êµ¬ë¶„í•˜ì—¬ ëª¨ë‘ ì œê³µí•˜ì„¸ìš”.\n"
        "4. **ì •ê·œì‹ íŒ¨í„´ì€ ë°˜ë“œì‹œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•˜ë©°, í‚¤ì›Œë“œë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ë‹¨ì–´**ì—¬ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: ['ì¸ê³µì§€ëŠ¥', 'AI', 'ë¨¸ì‹ ëŸ¬ë‹'])\n"
        "ì‘ë‹µì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì‘ë‹µ JSONì—ëŠ” **'upper_keyword'**, **'lower_keyword'**, **'regex_patterns'** í•„ë“œë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."
    )
    user_content = (
        f"ìƒì„¸ ë‚´ìš©(Content - Title, Content, ETC ì •ë³´ ê²°í•©ë¨): {cleaned_content}\n\n"
        "ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”."
    )

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME, 
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            response_format={"type": "json_object"},
            timeout=30.0
        )

        if not response.choices or not response.choices[0].message.content:
             return None, None, None

        response_text = response.choices[0].message.content
        result_dict = json.loads(response_text)

        upper_kw = result_dict.get('upper_keyword', None)
        lower_kw = result_dict.get('lower_keyword', None)
        regex_patterns = result_dict.get('regex_patterns', []) 

        if isinstance(upper_kw, list):
            upper_kw = ', '.join([str(k) for k in upper_kw])
        if isinstance(lower_kw, list):
            lower_kw = ', '.join([str(k) for k in lower_kw])
        
        if not isinstance(regex_patterns, list):
             regex_patterns = []

        return upper_kw, lower_kw, regex_patterns

    except APIError:
        pass
    except json.JSONDecodeError:
        pass
    except Exception:
        pass
        
    return None, None, None


# --- 3. JSON ê·œì¹™ ì¦‰ì‹œ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ ---

def update_rules_with_single_suggestion(
    rules: Dict[str, Dict[str, List[Any]]], 
    suggested_upper_str: str, 
    suggested_lower_str: str,
    regex_patterns: List[str],
    dynamic_lower_set: Set[str]
) -> Tuple[Dict[str, Dict[str, List[Any]]], Set[str], Optional[str]]:
    """
    ë‹¨ì¼ ì œì•ˆì„ ê¸°ì¡´ ê·œì¹™ ë”•ì…”ë„ˆë¦¬ì— ë³‘í•©í•˜ê³ , JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    suggested_upper = suggested_upper_str.split(', ')[0].strip()
    
    if not suggested_upper or not suggested_lower_str:
        return rules, dynamic_lower_set, None

    suggested_lower_raw = str(suggested_lower_str).replace('[', '').replace(']', '').replace("'", '').strip()
    if ', ' in suggested_lower_raw:
        lower_keywords = [kw.strip() for kw in suggested_lower_raw.split(', ') if kw.strip()]
    elif ',' in suggested_lower_raw:
        lower_keywords = [kw.strip() for kw in suggested_lower_raw.split(',') if kw.strip()]
    else:
        lower_keywords = [suggested_lower_raw] if suggested_lower_raw else []

    new_upper_generated = None
    merge_count = 0
    
    if suggested_upper not in rules:
        rules[suggested_upper] = {}
        new_upper_generated = suggested_upper
    
    for suggested_lower in lower_keywords:
        if not suggested_lower: continue

        if suggested_lower not in dynamic_lower_set: 
            if suggested_upper not in rules:
                 rules[suggested_upper] = {}

            rules[suggested_upper][suggested_lower] = regex_patterns # â­ï¸ ì •ê·œì‹ íŒ¨í„´ ì €ì¥ â­ï¸
            merge_count += 1
            dynamic_lower_set.add(suggested_lower)

    if merge_count > 0:
        try:
            os.makedirs(os.path.dirname(JSON_RULE_FILE), exist_ok=True)
            with open(JSON_RULE_FILE, 'w', encoding='utf-8') as f:
                json.dump(rules, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: JSON íŒŒì¼ ì €ì¥ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            
    return rules, dynamic_lower_set, new_upper_generated


# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì¦‰ì‹œ ë°˜ì˜ ë¡œì§ ë° ì›ë³¸ ì—…ë°ì´íŠ¸) ---
def process_unmapped_data():
    """ë¯¸ë§¤í•‘ëœ CSVë¥¼ ì²˜ë¦¬í•˜ê³ , ìƒì„±ëœ ê·œì¹™ì„ ì¦‰ì‹œ ë°˜ì˜í•˜ë©° ì›ë³¸ CSVì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""

    print("--- GPT API ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± ì‹œì‘ (ì¦‰ì‹œ ë°˜ì˜ ëª¨ë“œ) ---")

    # 0. íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(UNMAPPED_CSV_FILE):
        print(f"âŒ ì˜¤ë¥˜: ë¯¸ë§¤í•‘ ê²€í†  íŒŒì¼ '{UNMAPPED_CSV_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'mapper_keyword.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # 1. ì´ˆê¸° ê·œì¹™ ë¡œë“œ ë° ë™ì  ë³€ìˆ˜ ì´ˆê¸°í™”
    current_rules = load_simplified_rules(JSON_RULE_FILE)
    existing_upper_str, lower_str_avoid_init, dynamic_lower_keywords_set = extract_existing_keywords(current_rules)
    
    current_existing_upper_str = existing_upper_str
    
    # 2. ë¯¸ë§¤í•‘ CSV ë¡œë“œ
    df_unmapped = pd.read_csv(UNMAPPED_CSV_FILE)
    initial_rows = len(df_unmapped)
    print(f"âœ… ë¯¸ë§¤í•‘ í•­ëª© {initial_rows}ê°œ ë¡œë“œ ì™„ë£Œ. GPT API í˜¸ì¶œ ì‹œì‘...")

    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    suggested_upper_kws = []
    suggested_lower_kws = []
    
    # 3. GPT API í•¨ìˆ˜ ì ìš© (í–‰ ë‹¨ìœ„ ìˆœíšŒ ë° ì¦‰ì‹œ ë°˜ì˜)
    for index, row in df_unmapped.iterrows():
        # 3-1. 3ê°œ ì»¬ëŸ¼ ê²°í•©
        combined_text = combine_content_columns(row, TEXT_COLUMN_NAMES) 
        
        # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ í•˜ìœ„ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì—¬ GPTì—ê²Œ ì „ë‹¬í•  ê¸ˆì§€ ëª©ë¡ ë¬¸ìì—´ ìƒì„±
        current_lower_str_to_avoid = ', '.join(sorted(list(dynamic_lower_keywords_set)))
        
        # 3-2. GPT í˜¸ì¶œ (í˜„ì¬ì˜ ìµœì‹  ê·œì¹™/ëª©ë¡ ë°˜ì˜)
        upper_kw, lower_kw, regex_patterns = generate_keywords(
            combined_text, 
            current_existing_upper_str, 
            current_lower_str_to_avoid  
        )

        suggested_upper_kws.append(upper_kw)
        suggested_lower_kws.append(lower_kw)

        # 3-3. â­ï¸ ì¦‰ì‹œ ë°˜ì˜ ë¡œì§ (í•µì‹¬) â­ï¸
        # GPTê°€ í‚¤ì›Œë“œë¥¼ ìƒì„±í–ˆê³ , íŒ¨í„´ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
        if upper_kw and lower_kw and regex_patterns:
            
            current_rules, dynamic_lower_keywords_set, new_upper_name = \
                update_rules_with_single_suggestion(
                    current_rules,
                    upper_kw,
                    lower_kw,
                    regex_patterns, # â­ï¸ ì •ê·œì‹ íŒ¨í„´ ì „ë‹¬ â­ï¸
                    dynamic_lower_keywords_set
                )
            
            # ë‹¤ìŒ í–‰ì˜ GPT í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©ë  'ê¸°ì¡´ ìƒìœ„ í‚¤ì›Œë“œ' ëª©ë¡ ì—…ë°ì´íŠ¸
            if new_upper_name:
                current_existing_upper_set = set(current_rules.keys())
                current_existing_upper_str = ', '.join(sorted(list(current_existing_upper_set)))
                print(f"ğŸ’¡ ì‹ ê·œ ìƒìœ„ í‚¤ì›Œë“œ '{new_upper_name}' ì¦‰ì‹œ ë°˜ì˜.")


    # ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ ë°˜ì˜
    df_unmapped['upper_keyword_new'] = suggested_upper_kws # ì„ì‹œ ì»¬ëŸ¼ëª… ì‚¬ìš©
    df_unmapped['lower_keyword_new'] = suggested_lower_kws # ì„ì‹œ ì»¬ëŸ¼ëª… ì‚¬ìš©

    # 4. ê²°ê³¼ ë¶„ë¥˜ ë° ì›ë³¸ CSV ì—…ë°ì´íŠ¸
    
    # 4-1. GPTê°€ í‚¤ì›Œë“œë¥¼ ìƒì„±í•œ í–‰ê³¼ ì‹¤íŒ¨í•œ í–‰ ë¶„ë¦¬
    df_unmapped['is_fixed'] = df_unmapped['upper_keyword_new'].notna()
    df_fixed_by_gpt = df_unmapped[df_unmapped['is_fixed']].copy()
    
    # 4-2. ì›ë³¸ crawling_db.csv ì—…ë°ì´íŠ¸ (í•µì‹¬: _original_index_ ê¸°ì¤€ ë³‘í•©)
    if not df_fixed_by_gpt.empty:
        # DB íŒŒì¼ ë¡œë“œ
        df_original = pd.read_csv(ORIGINAL_CSV_FILE)
        
        # '_original_index_'ë¥¼ ì°¾ê±°ë‚˜ ìƒì„±í•˜ì—¬ ë³‘í•© í‚¤ë¡œ ì‚¬ìš©
        if '_original_index_' not in df_original.columns:
            df_original['_original_index_'] = df_original.index
            
        # ë³‘í•©ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        df_new_kws = df_fixed_by_gpt[['_original_index_', 'upper_keyword_new', 'lower_keyword_new']]
        
        # ì›ë³¸ DFì™€ GPT ê²°ê³¼ DFë¥¼ ë³‘í•©
        df_original = pd.merge(
            df_original, 
            df_new_kws, 
            on=['_original_index_'], 
            how='left'
        )
        
        # 2. ìµœì¢… í‚¤ì›Œë“œ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸: GPTê°€ ë§Œë“  ìƒˆ ê°’ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê³ , ì—†ìœ¼ë©´ ê¸°ì¡´ ê°’ ìœ ì§€
        df_original['upper_keyword'] = df_original['upper_keyword_new'].fillna(df_original.get('upper_keyword', ''))
        df_original['lower_keyword'] = df_original['lower_keyword_new'].fillna(df_original.get('lower_keyword', ''))
        
        # 3. ì„ì‹œ/ì¤‘ë³µ ì»¬ëŸ¼ ë° ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì‚­ì œ
        cols_to_drop = [col for col in df_original.columns if col.endswith('_new') or col == '_original_index_']
        df_original = df_original.drop(columns=cols_to_drop, errors='ignore')
        
        # 4. ì—…ë°ì´íŠ¸ëœ ì›ë³¸ DB íŒŒì¼ ì €ì¥
        os.makedirs(os.path.dirname(ORIGINAL_CSV_FILE), exist_ok=True)
        df_original.to_csv(ORIGINAL_CSV_FILE, index=False, encoding='utf-8-sig')
        print(f"[ì„±ê³µ] GPTë¡œ ë§¤í•‘ëœ í•­ëª© {len(df_fixed_by_gpt)}ê±´ì„ '{ORIGINAL_CSV_FILE}'ì— ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
    
    # 4-3. GPTë„ ë§¤í•‘ ëª»í•œ ì• ë“¤ ì €ì¥ (unmapped_crawling.csv íŒŒì¼ ì—…ë°ì´íŠ¸)
    # ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì»¬ëŸ¼ ì œê±° í›„ ì €ì¥
    cols_to_drop_unfixed = ['upper_keyword_new', 'lower_keyword_new', 'is_fixed']
    df_unfixed_by_gpt = df_unmapped[~df_unmapped['is_fixed']].drop(columns=cols_to_drop_unfixed, errors='ignore')
    
    os.makedirs(os.path.dirname(UNMAPPED_CSV_FILE), exist_ok=True)
    df_unfixed_by_gpt.to_csv(UNMAPPED_CSV_FILE, index=False, encoding='utf-8-sig')
    print(f"[ì‹¤íŒ¨] GPTë„ ë§¤í•‘í•˜ì§€ ëª»í•œ í•­ëª© {len(df_unfixed_by_gpt)}ê±´ì„ '{UNMAPPED_CSV_FILE}'ì— ë‹¤ì‹œ ì €ì¥í–ˆìŠµë‹ˆë‹¤. (ìˆ˜ë™ ê²€í†  í•„ìš”)")

    print("------------------------------------------------------------------")
    
    # â­ï¸â­ï¸â­ï¸ ì¶”ê°€ëœ ìƒ˜í”Œ ì¶œë ¥ ë¡œì§ â­ï¸â­ï¸â­ï¸
    if not df_fixed_by_gpt.empty:
        print("\n--- âœ… GPT ìƒì„±ìœ¼ë¡œ ì¶”ê°€ ë§¤í•‘ëœ ê²°ê³¼ ìƒ˜í”Œ (5ê±´) ---")
        preview_cols_fixed = ['title', 'upper_keyword_new', 'lower_keyword_new']
        # ì„ì‹œ ì»¬ëŸ¼ì´ë¯€ë¡œ df_unmappedì—ì„œ ì¶”ì¶œ í›„ ì¶œë ¥ (ì‹¤ì œ ì—…ë°ì´íŠ¸ ê°’ í™•ì¸)
        print(df_unmapped[df_unmapped['is_fixed']][preview_cols_fixed].head(5).to_markdown(index=False, numalign="left", stralign="left"))
    else:
        print("\n--- âœ… GPTë¡œ ì¶”ê°€ ë§¤í•‘ëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ---")


    if not df_unfixed_by_gpt.empty:
        print("\n--- âŒ GPTë„ ë§¤í•‘í•˜ì§€ ëª»í•œ ìµœì¢… ë¯¸í•´ê²° ëª©ë¡ ìƒ˜í”Œ (unmapped_crawling.csv) ---")
        # ì‹¤íŒ¨í•œ í–‰ì€ title, contentë§Œ ì¶œë ¥
        preview_cols_unfixed = ['title', 'content']
        print(df_unfixed_by_gpt[preview_cols_unfixed].head(5).to_markdown(index=False, numalign="left", stralign="left"))
    else:
        print("ë§¤í•‘ ì•ˆ ëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. í›Œë¥­í•©ë‹ˆë‹¤!")


# --- 5. ëª¨ë“ˆ ì‹¤í–‰ ì‹œ ë©”ì¸ ë¸”ë¡ ---

if __name__ == "__main__":
    load_dotenv()
    # 0. í•„ìˆ˜ í´ë” ìƒì„± í™•ì¸ (ê·œì¹™ íŒŒì¼ì˜ í´ë” ê²½ë¡œ)
    rules_dir = os.path.dirname(JSON_RULE_FILE)
    if rules_dir and not os.path.exists(rules_dir):
        os.makedirs(rules_dir, exist_ok=True)
        print(f"[{rules_dir}] í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    # 1. GPT í˜¸ì¶œì„ í†µí•´ ë¯¸ë§¤í•‘ëœ ë°ì´í„°ì— í‚¤ì›Œë“œ ì œì•ˆì„ ì¶”ê°€í•˜ê³  CSV ì €ì¥ (ì¦‰ì‹œ ë°˜ì˜ë¨)
    process_unmapped_data()